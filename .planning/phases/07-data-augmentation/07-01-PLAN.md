---
phase: 07-data-augmentation
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - study/data/augmentation.py
  - study/flow_matching/config.py
  - study/flow_matching/trainer.py
autonomous: true

must_haves:
  truths:
    - "Mixup generates interpolated embeddings with Beta(alpha, alpha) sampling"
    - "Gaussian noise adds configurable noise to embeddings"
    - "Augmentation only applies during training, not validation"
    - "Augmented embeddings maintain reasonable statistics (mean near 0, std near 1)"
  artifacts:
    - path: "study/data/augmentation.py"
      provides: "mixup_embeddings and add_gaussian_noise functions"
      exports: ["mixup_embeddings", "add_gaussian_noise", "AugmentationConfig"]
    - path: "study/flow_matching/config.py"
      provides: "Augmentation hyperparameters"
      contains: "mixup_alpha"
    - path: "study/flow_matching/trainer.py"
      provides: "Augmentation integration in train_epoch"
      contains: "augment_batch"
  key_links:
    - from: "study/flow_matching/trainer.py"
      to: "study/data/augmentation.py"
      via: "import and apply in train_epoch"
      pattern: "from study.data.augmentation import"
---

<objective>
Implement mixup (linear interpolation) and Gaussian noise injection augmentation for flow matching training.

Purpose: Enable data augmentation to improve generalization on small (1K-10K) SONAR embedding datasets.
Output: Augmentation module with mixup and noise functions integrated into training loop.
</objective>

<execution_context>
@/home/prusek/.claude/get-shit-done/workflows/execute-plan.md
@/home/prusek/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/07-data-augmentation/07-RESEARCH.md

@study/flow_matching/config.py
@study/flow_matching/trainer.py
@study/flow_matching/train.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create augmentation module with mixup and noise</name>
  <files>study/data/augmentation.py</files>
  <action>
Create `study/data/augmentation.py` with:

1. `AugmentationConfig` dataclass:
   - `mixup_alpha: float = 0.0` (0 = disabled, recommended 0.2)
   - `noise_std: float = 0.0` (0 = disabled, recommended 0.1)
   - `dropout_rate: float = 0.0` (placeholder for 07-02)

2. `mixup_embeddings(embeddings: Tensor, alpha: float) -> Tensor`:
   - Sample lambda from Beta(alpha, alpha) per sample
   - Create random permutation for pairing
   - Return linear interpolation: `lam * x + (1-lam) * x[indices]`
   - Use torch.distributions.Beta for sampling

3. `add_gaussian_noise(embeddings: Tensor, noise_std: float) -> Tensor`:
   - Add zero-mean Gaussian noise scaled by noise_std
   - Return embeddings + noise

4. `augment_batch(embeddings: Tensor, config: AugmentationConfig, training: bool) -> Tensor`:
   - Apply mixup if config.mixup_alpha > 0
   - Apply noise if config.noise_std > 0
   - Return original if not training
   - Order: mixup -> noise (as per research)

CRITICAL: Only augment x1 (data), never x0 (noise). Augmentation happens BEFORE coupling.sample().
  </action>
  <verify>
```python
import torch
from study.data.augmentation import mixup_embeddings, add_gaussian_noise, augment_batch, AugmentationConfig

# Test mixup
x = torch.randn(32, 1024)
mixed = mixup_embeddings(x, alpha=0.2)
assert mixed.shape == x.shape, "Mixup shape mismatch"

# Test noise
noisy = add_gaussian_noise(x, noise_std=0.1)
assert noisy.shape == x.shape, "Noise shape mismatch"
assert not torch.allclose(x, noisy), "Noise not applied"

# Test config
cfg = AugmentationConfig(mixup_alpha=0.2, noise_std=0.1)
augmented = augment_batch(x, cfg, training=True)
assert augmented.shape == x.shape, "Augment batch shape mismatch"

# Test training=False returns original
unchanged = augment_batch(x, cfg, training=False)
assert torch.allclose(x, unchanged), "Should not augment when training=False"

print("All augmentation tests passed!")
```
  </verify>
  <done>Augmentation module exists with mixup_embeddings, add_gaussian_noise, AugmentationConfig, and augment_batch functions that preserve tensor shapes and only apply during training.</done>
</task>

<task type="auto">
  <name>Task 2: Add augmentation config to TrainingConfig</name>
  <files>study/flow_matching/config.py</files>
  <action>
Extend `TrainingConfig` with augmentation hyperparameters:

1. Add new fields (locked defaults, do not show in repr):
   - `mixup_alpha: float = field(default=0.0, repr=False)`
   - `noise_std: float = field(default=0.0, repr=False)`
   - `dropout_rate: float = field(default=0.0, repr=False)` (for 07-02)

2. Update `to_dict()` to include new fields for Wandb logging

3. The existing `aug: str` field remains for run naming convention.
   Parse aug string in trainer to set individual params if needed.

Note: Individual hyperparameters allow fine-grained sweeps while `aug` string keeps run names readable.
  </action>
  <verify>
```python
from study.flow_matching.config import TrainingConfig

cfg = TrainingConfig(arch="mlp", flow="icfm", dataset="1k", aug="mixup", group="test")
d = cfg.to_dict()
assert "mixup_alpha" in d, "mixup_alpha missing from to_dict"
assert "noise_std" in d, "noise_std missing from to_dict"
assert "dropout_rate" in d, "dropout_rate missing from to_dict"
print(f"Config fields present: mixup_alpha={d['mixup_alpha']}, noise_std={d['noise_std']}")
```
  </verify>
  <done>TrainingConfig includes mixup_alpha, noise_std, and dropout_rate fields with defaults of 0.0, all included in to_dict() for Wandb logging.</done>
</task>

<task type="auto">
  <name>Task 3: Integrate augmentation into FlowTrainer</name>
  <files>study/flow_matching/trainer.py, study/flow_matching/train.py</files>
  <action>
Modify FlowTrainer to apply augmentation during training:

**PREREQUISITE VERIFICATION:** Confirm train_epoch method exists at line ~200 in trainer.py:
```bash
grep -n "def train_epoch" study/flow_matching/trainer.py
# Expected: 200:    def train_epoch(self) -> float:
```

1. In `FlowTrainer.__init__`:
   - Import AugmentationConfig from study.data.augmentation
   - Create self.aug_config from config parameters if any augmentation enabled
   - Parse config.aug string to set defaults:
     - "none" -> all 0.0
     - "mixup" -> mixup_alpha=0.2
     - "noise" -> noise_std=0.1
     - "mixup+noise" -> mixup_alpha=0.2, noise_std=0.1
   - Override with explicit config values if provided

2. In `train_epoch` (line ~200, integration point at line ~211-214):
   - Find the line: `x1 = x1.to(self.device)` (line ~211)
   - Add IMMEDIATELY AFTER that line, BEFORE `x0 = torch.randn_like(x1)` (line ~214):
     ```python
     # Apply augmentation (only during training)
     if self.aug_config is not None:
         x1 = augment_batch(x1, self.aug_config, training=True)
     ```
   - This ensures augmentation happens to x1 BEFORE coupling.sample()

3. In `validate` (line ~271):
   - Do NOT apply augmentation (training=False path - no changes needed since we only augment in train_epoch)

4. Update train.py CLI:
   - Add `--mixup-alpha`, `--noise-std` arguments (optional, override defaults)
   - Pass to TrainingConfig constructor
  </action>
  <verify>
Run a quick 2-epoch training with augmentation to verify integration:
```bash
cd /home/prusek/NLP
CUDA_VISIBLE_DEVICES=1 WANDB_MODE=offline uv run python -m study.flow_matching.train \
  --arch mlp --flow icfm --dataset 1k --aug mixup --group test-aug \
  --epochs 2 --batch-size 64
```
Check training completes without error and loss is logged.
  </verify>
  <done>FlowTrainer applies augmentation in train_epoch before coupling.sample(), augmentation is skipped during validation, and CLI supports --mixup-alpha and --noise-std overrides.</done>
</task>

</tasks>

<verification>
Run validation script to confirm augmentation quality:

```python
import torch
from study.data.augmentation import AugmentationConfig, augment_batch, mixup_embeddings

# Load real data
from study.data.dataset import load_all_splits
train_ds, _, _ = load_all_splits("1k", return_normalized=True)
x = train_ds.normalized_embeddings[:64].clone()

# Test mixup statistics
cfg = AugmentationConfig(mixup_alpha=0.2)
mixed = augment_batch(x.clone(), cfg, training=True)
print(f"Original: mean={x.mean():.4f}, std={x.std():.4f}")
print(f"Mixed: mean={mixed.mean():.4f}, std={mixed.std():.4f}")
assert abs(mixed.mean()) < 0.5, "Mean too far from 0"
assert 0.5 < mixed.std() < 1.5, "Std too far from 1"

# Test noise statistics
cfg2 = AugmentationConfig(noise_std=0.1)
noisy = augment_batch(x.clone(), cfg2, training=True)
print(f"Noisy: mean={noisy.mean():.4f}, std={noisy.std():.4f}")
assert abs(noisy.mean()) < 0.5, "Mean too far from 0"

# Test cosine similarity (should be high for light augmentation)
import torch.nn.functional as F
sim = F.cosine_similarity(x, mixed, dim=1)
print(f"Mixup similarity: mean={sim.mean():.4f}, min={sim.min():.4f}")
assert sim.mean() > 0.7, "Mixup too aggressive"

print("\nAll augmentation quality checks passed!")
```
</verification>

<success_criteria>
1. Augmentation module exists at study/data/augmentation.py with all required functions
2. TrainingConfig includes augmentation hyperparameters with defaults of 0.0
3. FlowTrainer applies augmentation to x1 before coupling.sample() during training only
4. Quick training run with --aug mixup completes without errors
5. Augmented embeddings maintain reasonable statistics (mean near 0, std near 1)
</success_criteria>

<output>
After completion, create `.planning/phases/07-data-augmentation/07-01-SUMMARY.md`
</output>
