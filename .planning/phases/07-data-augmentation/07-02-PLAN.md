---
phase: 07-data-augmentation
plan: 02
type: execute
wave: 2
depends_on: ["07-01"]
files_modified:
  - study/data/augmentation.py
  - study/flow_matching/train.py
autonomous: true

must_haves:
  truths:
    - "Dimension dropout zeros out dimensions using F.dropout"
    - "Combined augmentations (mixup+noise+dropout) work together"
    - "Augmented training improves generalization (lower val loss than baseline)"
  artifacts:
    - path: "study/data/augmentation.py"
      provides: "Complete augmentation module with all three strategies"
      exports: ["dimension_dropout", "augment_batch"]
    - path: "study/flow_matching/train.py"
      provides: "CLI with --dropout-rate argument"
      contains: "--dropout-rate"
  key_links:
    - from: "study/flow_matching/trainer.py"
      to: "study/data/augmentation.py"
      via: "augment_batch with all three strategies"
      pattern: "augment_batch.*training=True"
---

<objective>
Complete dimension dropout implementation and run ablation to verify augmentation improves generalization.

Purpose: Complete all three augmentation strategies and validate they improve training outcomes.
Output: Full augmentation module with validation that augmented training reduces val loss.
</objective>

<execution_context>
@/home/prusek/.claude/get-shit-done/workflows/execute-plan.md
@/home/prusek/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/07-data-augmentation/07-RESEARCH.md
@.planning/phases/07-data-augmentation/07-01-SUMMARY.md

@study/data/augmentation.py
@study/flow_matching/config.py
@study/flow_matching/trainer.py
@study/flow_matching/train.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Add dimension dropout to augmentation module</name>
  <files>study/data/augmentation.py</files>
  <action>
Add dimension dropout function and update augment_batch:

1. Add `dimension_dropout(embeddings: Tensor, dropout_rate: float, training: bool) -> Tensor`:
   - Use `F.dropout(embeddings, p=dropout_rate, training=training)`
   - F.dropout handles scaling by 1/(1-p) automatically
   - Return unchanged if training=False or dropout_rate=0.0

2. Update `augment_batch`:
   - Apply dropout after mixup and noise
   - Order: mixup -> noise -> dropout
   - Use config.dropout_rate

3. Add helper function for aug string parsing:
   ```python
   def parse_aug_string(aug: str) -> AugmentationConfig:
       """Parse aug string like 'mixup', 'noise', 'mixup+noise', 'all'."""
       if aug == "none":
           return AugmentationConfig()
       elif aug == "mixup":
           return AugmentationConfig(mixup_alpha=0.2)
       elif aug == "noise":
           return AugmentationConfig(noise_std=0.1)
       elif aug == "dropout":
           return AugmentationConfig(dropout_rate=0.1)
       elif aug == "mixup+noise":
           return AugmentationConfig(mixup_alpha=0.2, noise_std=0.1)
       elif aug == "all":
           return AugmentationConfig(mixup_alpha=0.2, noise_std=0.1, dropout_rate=0.1)
       else:
           return AugmentationConfig()  # Unknown, return no augmentation
   ```
  </action>
  <verify>
```python
import torch
import torch.nn.functional as F
from study.data.augmentation import dimension_dropout, augment_batch, AugmentationConfig, parse_aug_string

# Test dropout
x = torch.ones(32, 1024)  # All ones for easy verification
dropped = dimension_dropout(x, dropout_rate=0.1, training=True)
assert dropped.shape == x.shape, "Dropout shape mismatch"
# Some values should be zero, some should be scaled
zero_frac = (dropped == 0).float().mean().item()
print(f"Dropout zero fraction: {zero_frac:.3f} (expected ~0.1)")
assert 0.05 < zero_frac < 0.2, f"Dropout rate wrong: {zero_frac}"

# Test parse_aug_string
cfg = parse_aug_string("all")
assert cfg.mixup_alpha == 0.2, "Mixup alpha wrong"
assert cfg.noise_std == 0.1, "Noise std wrong"
assert cfg.dropout_rate == 0.1, "Dropout rate wrong"

# Test full augment_batch with all three
cfg = AugmentationConfig(mixup_alpha=0.2, noise_std=0.1, dropout_rate=0.1)
x = torch.randn(32, 1024)
augmented = augment_batch(x.clone(), cfg, training=True)
assert augmented.shape == x.shape, "Full augmentation shape mismatch"
print("All dropout tests passed!")
```
  </verify>
  <done>dimension_dropout function uses F.dropout with proper scaling, augment_batch applies all three in order (mixup -> noise -> dropout), parse_aug_string helper exists.</done>
</task>

<task type="auto">
  <name>Task 2: Update CLI with dropout argument</name>
  <files>study/flow_matching/train.py</files>
  <action>
Add `--dropout-rate` CLI argument:

1. Add argument:
   ```python
   parser.add_argument(
       "--dropout-rate",
       type=float,
       default=0.0,
       help="Dimension dropout rate (0.0 = disabled, recommended 0.1)",
   )
   ```

2. Pass to TrainingConfig:
   ```python
   dropout_rate=args.dropout_rate,
   ```

3. Update FlowTrainer._setup to use parse_aug_string from augmentation module
   instead of inline parsing (cleaner code organization).
  </action>
  <verify>
```bash
cd /home/prusek/NLP
uv run python -m study.flow_matching.train --help | grep dropout
```
Should show `--dropout-rate` option.
  </verify>
  <done>CLI supports --dropout-rate argument that flows through to TrainingConfig and FlowTrainer.</done>
</task>

<task type="auto">
  <name>Task 3: Run ablation comparing augmentation methods</name>
  <files>None (runs existing code)</files>
  <action>
Run short training ablation to verify augmentation improves generalization:

1. Baseline (no augmentation):
```bash
CUDA_VISIBLE_DEVICES=1 WANDB_MODE=offline uv run python -m study.flow_matching.train \
  --arch mlp --flow icfm --dataset 1k --aug none --group ablation-aug --epochs 30
```

2. Mixup only:
```bash
CUDA_VISIBLE_DEVICES=1 WANDB_MODE=offline uv run python -m study.flow_matching.train \
  --arch mlp --flow icfm --dataset 1k --aug mixup --group ablation-aug --epochs 30
```

3. Noise only:
```bash
CUDA_VISIBLE_DEVICES=1 WANDB_MODE=offline uv run python -m study.flow_matching.train \
  --arch mlp --flow icfm --dataset 1k --aug noise --group ablation-aug --epochs 30
```

4. Combined (mixup+noise):
```bash
CUDA_VISIBLE_DEVICES=1 WANDB_MODE=offline uv run python -m study.flow_matching.train \
  --arch mlp --flow icfm --dataset 1k --aug mixup+noise --group ablation-aug --epochs 30
```

Extract final val loss from each run and compare.

CRITICAL: Use tmux for runs >30 seconds. Each run takes ~5-10 minutes.

Expected outcome: At least one augmented run should have lower val loss than baseline.
If augmentation hurts (higher val loss), this is still a valid finding - document it.
  </action>
  <verify>
After all runs complete, extract and compare val losses:
```bash
# Check checkpoints for best_loss values
cd /home/prusek/NLP
for ckpt in study/checkpoints/mlp-icfm-1k-*/best.pt; do
  echo "$ckpt:"
  python -c "import torch; d=torch.load('$ckpt', map_location='cpu'); print(f\"  best_loss: {d.get('best_loss', 'N/A')}\")"
done
```
  </verify>
  <done>Ablation runs complete with val loss comparison. Results documented showing whether augmentation improves generalization. Any outcome (improvement or not) is valid research finding.</done>
</task>

</tasks>

<verification>
Complete verification of Phase 7 success criteria:

1. **Mixup generates valid training pairs:**
```python
import torch
from study.data.augmentation import mixup_embeddings
from study.data.dataset import load_all_splits
import torch.nn.functional as F

train_ds, _, _ = load_all_splits("1k", return_normalized=True)
x = train_ds.normalized_embeddings[:64].clone()
mixed = mixup_embeddings(x, alpha=0.2)

# Check similarity (should be high for light mixup)
sim = F.cosine_similarity(x, mixed, dim=1)
assert sim.mean() > 0.7, f"Mixup too aggressive: {sim.mean():.4f}"
print(f"Mixup similarity: {sim.mean():.4f} (pass: >0.7)")
```

2. **Gaussian noise augments training data:**
```python
from study.data.augmentation import add_gaussian_noise

noisy = add_gaussian_noise(x, noise_std=0.1)
assert not torch.allclose(x, noisy), "Noise not applied"
# Check stats preserved
assert abs(noisy.mean()) < 0.5, f"Mean shifted: {noisy.mean():.4f}"
assert 0.5 < noisy.std() < 1.5, f"Std wrong: {noisy.std():.4f}"
print(f"Noisy stats: mean={noisy.mean():.4f}, std={noisy.std():.4f} (pass)")
```

3. **Dimension dropout/masking augments training data:**
```python
from study.data.augmentation import dimension_dropout

dropped = dimension_dropout(torch.ones(32, 1024), dropout_rate=0.1, training=True)
zero_frac = (dropped == 0).float().mean().item()
assert 0.05 < zero_frac < 0.2, f"Dropout wrong: {zero_frac}"
print(f"Dropout zero fraction: {zero_frac:.3f} (pass: ~0.1)")
```

4. **Augmented training improves generalization:**
Compare val losses from ablation runs. Document in SUMMARY.md.
</verification>

<success_criteria>
1. dimension_dropout function exists and uses F.dropout with proper scaling
2. augment_batch applies all three augmentations in order (mixup -> noise -> dropout)
3. CLI supports --dropout-rate argument
4. Ablation runs complete comparing baseline vs augmented training
5. Results documented showing impact of augmentation on val loss
</success_criteria>

<output>
After completion, create `.planning/phases/07-data-augmentation/07-02-SUMMARY.md`

Include:
- Ablation results table (aug method vs val loss)
- Whether augmentation improved generalization
- Recommended augmentation settings for future experiments
- Phase 7 verification checklist
</output>
