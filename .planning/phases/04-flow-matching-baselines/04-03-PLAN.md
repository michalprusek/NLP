---
phase: 04-flow-matching-baselines
plan: 03
type: execute
wave: 2
depends_on: ["04-01"]
files_modified:
  - study/flow_matching/guidance.py
  - study/flow_matching/evaluate.py
autonomous: true

must_haves:
  truths:
    - "CFG-Zero* schedule zeros guidance for first 4% of steps"
    - "Guided sampling function available for future GP integration"
    - "Both I-CFM and OT-CFM compatible with CFG-Zero* guidance"
  artifacts:
    - path: "study/flow_matching/guidance.py"
      provides: "CFG-Zero* guidance utilities"
      exports: ["get_guidance_lambda", "guided_euler_ode_integrate"]
      min_lines: 60
  key_links:
    - from: "study/flow_matching/guidance.py"
      to: "rielbo/guided_flow.py"
      via: "CFG-Zero* schedule pattern"
      pattern: "zero_init_fraction.*0.04"
---

<objective>
Implement CFG-Zero* guidance schedule for the study module, preparing for GP-guided sampling.

Purpose: CFG-Zero* (zero guidance for first 4% of steps) prevents early trajectory corruption and improves guided sample quality. This prepares the study module for Phase 8 GP-guided sampling integration.

Output: Guidance module with CFG-Zero* schedule, guided ODE integration function.
</objective>

<execution_context>
@/home/prusek/.claude/get-shit-done/workflows/execute-plan.md
@/home/prusek/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/04-flow-matching-baselines/04-RESEARCH.md
@rielbo/guided_flow.py

Key context from rielbo/guided_flow.py:
- CFG-Zero* already implemented at lines 52-57
- zero_init_fraction = 0.04 (4% of steps)
- Formula: if step < zero_init_steps: return 0.0 else: return guidance_strength
- Gradient clipping with max_grad_norm = 10.0
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create guidance module with CFG-Zero* schedule</name>
  <files>study/flow_matching/guidance.py</files>
  <action>
Create study/flow_matching/guidance.py with CFG-Zero* utilities:

```python
"""CFG-Zero* guidance utilities for flow matching.

Implements the CFG-Zero* schedule from arXiv:2503.18886 which zeros guidance
for the first 4% of ODE integration steps to prevent early trajectory corruption.

This module provides:
- get_guidance_lambda(): CFG-Zero* schedule function
- guided_euler_ode_integrate(): ODE integration with optional guidance

For actual GP-guided sampling, see Phase 8 which integrates with GP surrogate.
"""

import logging
from typing import Callable, Optional

import torch
from torch import Tensor
from tqdm import tqdm

logger = logging.getLogger(__name__)


def get_guidance_lambda(
    step: int,
    total_steps: int,
    guidance_strength: float,
    zero_init_fraction: float = 0.04,
) -> float:
    """Get guidance strength using CFG-Zero* schedule.

    CFG-Zero* zeros guidance for the first 4% of steps to prevent
    early trajectory corruption when the flow estimate is inaccurate.

    Args:
        step: Current step (0-indexed)
        total_steps: Total number of ODE steps
        guidance_strength: Maximum guidance strength (lambda)
        zero_init_fraction: Fraction of steps with zero guidance (default 0.04 = 4%)

    Returns:
        Guidance strength for this step (0.0 during zero-init period)
    """
    zero_init_steps = max(1, int(zero_init_fraction * total_steps))
    if step < zero_init_steps:
        return 0.0
    return guidance_strength


@torch.no_grad()
def guided_euler_ode_integrate(
    model: torch.nn.Module,
    x0: Tensor,
    n_steps: int,
    device: torch.device,
    guidance_fn: Optional[Callable[[Tensor], Tensor]] = None,
    guidance_strength: float = 1.0,
    zero_init_fraction: float = 0.04,
    grad_clip_norm: float = 10.0,
    show_progress: bool = True,
) -> Tensor:
    """
    Integrate ODE from t=0 to t=1 with optional CFG-Zero* guidance.

    The flow ODE is: dx/dt = v(x, t) + lambda(t) * grad_guidance(x)
    where lambda(t) follows the CFG-Zero* schedule (zero for first 4% of steps).

    Args:
        model: Velocity network with forward(x, t) -> v signature.
        x0: Initial points at t=0, shape [N, D].
        n_steps: Number of integration steps.
        device: Computation device.
        guidance_fn: Optional function that computes guidance gradient.
                    Should accept x [N, D] and return gradient [N, D].
                    If None, no guidance is applied.
        guidance_strength: Maximum guidance strength lambda.
        zero_init_fraction: Fraction of steps with zero guidance.
        grad_clip_norm: Maximum norm for guidance gradient clipping.
        show_progress: Whether to show tqdm progress bar.

    Returns:
        x1: Points at t=1, shape [N, D].
    """
    dt = 1.0 / n_steps
    x = x0.to(device)
    model.eval()

    iterator = range(n_steps)
    if show_progress:
        iterator = tqdm(iterator, desc="Guided ODE integration", leave=False)

    for i in iterator:
        t = i / n_steps
        t_batch = torch.full((x.shape[0],), t, device=device, dtype=x.dtype)

        # Base velocity
        v = model(x, t_batch)

        # Add guidance (with CFG-Zero* schedule)
        if guidance_fn is not None:
            lambda_t = get_guidance_lambda(i, n_steps, guidance_strength, zero_init_fraction)
            if lambda_t > 0:
                grad = guidance_fn(x)

                # Gradient clipping
                grad_norm = grad.norm(dim=-1, keepdim=True)
                clip_mask = grad_norm > grad_clip_norm
                if clip_mask.any():
                    grad = torch.where(
                        clip_mask,
                        grad * grad_clip_norm / grad_norm,
                        grad
                    )

                v = v + lambda_t * grad

        # Euler step
        x = x + dt * v

    return x


@torch.no_grad()
def sample_with_guidance(
    model: torch.nn.Module,
    n_samples: int,
    n_steps: int,
    device: str | torch.device,
    guidance_fn: Optional[Callable[[Tensor], Tensor]] = None,
    guidance_strength: float = 1.0,
    zero_init_fraction: float = 0.04,
) -> Tensor:
    """
    Generate samples from flow model with optional CFG-Zero* guidance.

    Args:
        model: Velocity network in eval mode.
        n_samples: Number of samples to generate.
        n_steps: Number of ODE integration steps.
        device: Computation device.
        guidance_fn: Optional guidance gradient function.
        guidance_strength: Maximum guidance strength.
        zero_init_fraction: Fraction of steps with zero guidance.

    Returns:
        Generated samples [n_samples, 1024] in normalized space.
    """
    device = torch.device(device) if isinstance(device, str) else device

    # Start from noise
    x0 = torch.randn(n_samples, 1024, device=device)

    # Integrate with guidance
    x1 = guided_euler_ode_integrate(
        model=model,
        x0=x0,
        n_steps=n_steps,
        device=device,
        guidance_fn=guidance_fn,
        guidance_strength=guidance_strength,
        zero_init_fraction=zero_init_fraction,
        show_progress=True,
    )

    return x1


# Example guidance function for testing
def make_random_guidance_fn(target: Tensor) -> Callable[[Tensor], Tensor]:
    """Create a simple guidance function toward a target embedding.

    This is for testing only. Real guidance uses GP-UCB gradient.

    Args:
        target: Target embedding [1, D] or [D] to guide toward.

    Returns:
        Guidance function that returns gradient toward target.
    """
    if target.dim() == 1:
        target = target.unsqueeze(0)

    def guidance_fn(x: Tensor) -> Tensor:
        """Gradient toward target (normalized)."""
        diff = target.to(x.device) - x
        # Normalize to unit length for stable guidance
        diff_norm = diff.norm(dim=-1, keepdim=True).clamp(min=1e-8)
        return diff / diff_norm

    return guidance_fn
```
  </action>
  <verify>
```bash
CUDA_VISIBLE_DEVICES=1 uv run python -c "
import torch
from study.flow_matching.guidance import (
    get_guidance_lambda,
    guided_euler_ode_integrate,
    sample_with_guidance,
    make_random_guidance_fn
)
from study.flow_matching.evaluate import load_checkpoint

# Test CFG-Zero* schedule
assert get_guidance_lambda(0, 100, 1.0, 0.04) == 0.0, 'Step 0 should have zero guidance'
assert get_guidance_lambda(3, 100, 1.0, 0.04) == 0.0, 'Step 3 should have zero guidance'
assert get_guidance_lambda(4, 100, 1.0, 0.04) == 1.0, 'Step 4 should have full guidance'
assert get_guidance_lambda(50, 100, 2.5, 0.04) == 2.5, 'Step 50 should have full guidance'
print('CFG-Zero* schedule tests passed')

# Test guided sampling (without guidance - should work like regular sampling)
model, stats = load_checkpoint('study/checkpoints/mlp-icfm-1k-none/best.pt', 'mlp', 'cuda:0')
samples = sample_with_guidance(
    model=model,
    n_samples=5,
    n_steps=50,
    device='cuda:0',
    guidance_fn=None,  # No guidance
)
assert samples.shape == (5, 1024), f'Expected (5, 1024), got {samples.shape}'
print('Unguided sampling works')

# Test with simple guidance function
target = torch.randn(1, 1024, device='cuda:0')
guidance_fn = make_random_guidance_fn(target)
samples = sample_with_guidance(
    model=model,
    n_samples=5,
    n_steps=50,
    device='cuda:0',
    guidance_fn=guidance_fn,
    guidance_strength=0.5,
)
assert samples.shape == (5, 1024)
print('Guided sampling works')

print('All guidance module tests passed')
"
```
  </verify>
  <done>
    - get_guidance_lambda() implements CFG-Zero* schedule (4% zero-init)
    - guided_euler_ode_integrate() supports optional guidance with gradient clipping
    - sample_with_guidance() convenience function for generation
    - Works with both I-CFM and OT-CFM models
  </done>
</task>

<task type="auto">
  <name>Task 2: Verify CFG-Zero* integration and document phase completion</name>
  <files>None (verification only)</files>
  <action>
Comprehensive verification of Phase 4 success criteria:

1. **Verify I-CFM trains with independent coupling:**
```bash
# Already verified in Phase 3, confirm checkpoint exists
ls -la study/checkpoints/mlp-icfm-1k-none/best.pt
```

2. **Verify OT-CFM trains with Sinkhorn coupling:**
```bash
# Trained in 04-02, confirm checkpoint exists
ls -la study/checkpoints/mlp-otcfm-1k-none/best.pt
```

3. **Compare path straightness (OT-CFM should be straighter):**
```bash
CUDA_VISIBLE_DEVICES=1 uv run python -c "
import torch
from study.flow_matching.evaluate import load_checkpoint, compute_path_straightness

test_data = torch.load('study/datasets/splits/1k/test.pt', weights_only=False)
test_emb = test_data['embeddings']

# I-CFM
model_icfm, _ = load_checkpoint('study/checkpoints/mlp-icfm-1k-none/best.pt', 'mlp', 'cuda:0')
icfm_results = compute_path_straightness(model_icfm, test_emb, n_samples=50, n_steps=50, device='cuda:0')

# OT-CFM
model_otcfm, _ = load_checkpoint('study/checkpoints/mlp-otcfm-1k-none/best.pt', 'mlp', 'cuda:0')
otcfm_results = compute_path_straightness(model_otcfm, test_emb, n_samples=50, n_steps=50, device='cuda:0')

print('Path Straightness Comparison:')
print(f'  I-CFM:  mean={icfm_results[\"mean_path_deviation\"]:.4f}, var={icfm_results[\"path_variance\"]:.6f}')
print(f'  OT-CFM: mean={otcfm_results[\"mean_path_deviation\"]:.4f}, var={otcfm_results[\"path_variance\"]:.6f}')

if otcfm_results['mean_path_deviation'] < icfm_results['mean_path_deviation']:
    print('SUCCESS: OT-CFM produces straighter paths than I-CFM')
else:
    print('NOTE: Path straightness comparison - may need more training')
"
```

4. **Verify CFG-Zero* zeros first 4% of steps:**
```bash
CUDA_VISIBLE_DEVICES=1 uv run python -c "
from study.flow_matching.guidance import get_guidance_lambda

# Test at various step counts
for n_steps in [50, 100, 200]:
    zero_steps = max(1, int(0.04 * n_steps))
    print(f'n_steps={n_steps}: first {zero_steps} steps have zero guidance')
    for step in range(zero_steps + 2):
        lam = get_guidance_lambda(step, n_steps, 1.0, 0.04)
        expected = 0.0 if step < zero_steps else 1.0
        assert lam == expected, f'Step {step}: expected {expected}, got {lam}'
print('CFG-Zero* schedule verified correct')
"
```

5. **Verify both methods generate valid SONAR embeddings:**
```bash
CUDA_VISIBLE_DEVICES=1 uv run python -c "
import torch
from study.flow_matching.evaluate import load_checkpoint, generate_and_decode
from study.data.normalize import load_stats
from rielbo.decoder import SonarDecoder

decoder = SonarDecoder(device='cuda:0')
stats = load_stats('study/datasets/normalization_stats.pt')

for flow_type in ['icfm', 'otcfm']:
    checkpoint = f'study/checkpoints/mlp-{flow_type}-1k-none/best.pt'
    model, _ = load_checkpoint(checkpoint, 'mlp', 'cuda:0')
    texts = generate_and_decode(model, stats, decoder, n_samples=3, n_steps=50, device='cuda:0')
    print(f'{flow_type.upper()} generated texts:')
    for i, text in enumerate(texts, 1):
        print(f'  {i}. \"{text}\"')
"
```
  </action>
  <verify>
All verification commands above should complete successfully, showing:
- Both checkpoints exist
- OT-CFM has lower path deviation than I-CFM
- CFG-Zero* correctly zeros first 4% of steps
- Both methods decode to coherent English text
  </verify>
  <done>
    - I-CFM trains with independent coupling (checkpoint verified)
    - OT-CFM trains with Sinkhorn coupling (checkpoint verified)
    - OT-CFM produces straighter paths (lower deviation)
    - CFG-Zero* zeros first 4% of steps (schedule verified)
    - Both methods generate valid SONAR embeddings (text quality verified)
  </done>
</task>

</tasks>

<verification>
Final Phase 4 success criteria checklist:

1. [x] I-CFM trains with independent noise-data coupling
   - Verified by existing mlp-icfm-1k-none checkpoint

2. [x] OT-CFM trains with mini-batch Sinkhorn coupling
   - torchcfm.ExactOptimalTransportConditionalFlowMatcher handles Sinkhorn internally

3. [ ] OT-CFM produces straighter paths than I-CFM
   - Measure mean_path_deviation for both models
   - OT-CFM deviation < I-CFM deviation

4. [x] CFG-Zero* schedule zeros guidance for first 4% of steps
   - get_guidance_lambda() returns 0.0 for step < zero_init_steps

5. [ ] Both methods generate valid SONAR embeddings
   - Generated embeddings decode to coherent English text
</verification>

<success_criteria>
- [ ] guidance.py module created with CFG-Zero* schedule
- [ ] get_guidance_lambda() correctly implements 4% zero-init
- [ ] guided_euler_ode_integrate() supports optional guidance with clipping
- [ ] All Phase 4 success criteria verified and documented
- [ ] Phase ready for Phase 5 (Advanced Flow Methods) and Phase 8 (GP-Guided Sampling)
</success_criteria>

<output>
After completion, create `.planning/phases/04-flow-matching-baselines/04-03-SUMMARY.md`

Include:
- CFG-Zero* schedule implementation details
- Final path straightness comparison table
- Text generation samples from both methods
- Phase 4 completion summary
</output>
