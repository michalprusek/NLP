---
phase: 06-advanced-architectures
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - study/flow_matching/models/unet_mlp.py
  - study/flow_matching/models/__init__.py
autonomous: true

must_haves:
  truths:
    - "UNetMLP architecture with FiLM conditioning trains without NaN loss"
    - "Skip connections preserve embedding information through encoder-decoder"
    - "FiLM layers modulate features based on timestep"
    - "Model produces valid velocity predictions with shape [B, 1024]"
  artifacts:
    - path: "study/flow_matching/models/unet_mlp.py"
      provides: "UNetMLP and FiLMLayer classes"
      exports: ["UNetMLP", "FiLMLayer"]
    - path: "study/flow_matching/models/__init__.py"
      provides: "Extended factory with unet support"
      contains: "unet"
  key_links:
    - from: "study/flow_matching/models/unet_mlp.py"
      to: "study/flow_matching/models/mlp.py"
      via: "import timestep_embedding"
      pattern: "from.*mlp import timestep_embedding"
    - from: "study/flow_matching/models/__init__.py"
      to: "study/flow_matching/models/unet_mlp.py"
      via: "import and factory"
      pattern: "from.*unet_mlp import UNetMLP"
---

<objective>
Implement U-Net MLP velocity network with FiLM (Feature-wise Linear Modulation) time conditioning.

Purpose: U-Net architecture with skip connections may better preserve embedding structure through the encoder-decoder pipeline compared to simple MLP. FiLM provides efficient time conditioning via affine modulation (gamma*x + beta).

Output: UNetMLP class (~2.5M params with default config) integrated into model factory.
</objective>

<execution_context>
@/home/prusek/.claude/get-shit-done/workflows/execute-plan.md
@/home/prusek/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/06-advanced-architectures/06-RESEARCH.md

Reference files:
@study/flow_matching/models/__init__.py
@study/flow_matching/models/mlp.py
@study/flow_matching/models/dit.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Implement FiLMLayer and UNetMLP</name>
  <files>study/flow_matching/models/unet_mlp.py</files>
  <action>
Create study/flow_matching/models/unet_mlp.py with:

1. **FiLMLayer class** - Feature-wise Linear Modulation:
   - Input: feature_dim, conditioning_dim
   - film_params: nn.Linear(conditioning_dim, 2 * feature_dim) for gamma and beta
   - CRITICAL: Initialize to identity transform (gamma=1, beta=0):
     - nn.init.zeros_(self.film_params.weight)
     - nn.init.zeros_(self.film_params.bias)
     - self.film_params.bias.data[:feature_dim] = 1.0  # gamma = 1
   - forward(x, cond) returns gamma * x + beta

2. **UNetMLPBlock class** - Single encoder/decoder block:
   - linear1: in_dim -> out_dim
   - linear2: out_dim -> out_dim
   - film: FiLMLayer(out_dim, cond_dim)
   - act: nn.SiLU()
   - Residual connection if in_dim == out_dim
   - forward(x, cond): act(linear1) -> film -> act(linear2) -> residual

3. **UNetMLP class** - Main velocity network (~2.5M params):
   - input_dim=1024, hidden_dims=(512, 256), time_embed_dim=256
   - time_mlp: Linear -> SiLU -> Linear (time_embed_dim -> time_embed_dim)
   - encoder: ModuleList of UNetMLPBlock [1024->512, 512->256]
   - decoder: ModuleList of UNetMLPBlock [256->512, 512*2->1024] (accounting for skip concat)
   - output_proj: Linear(input_dim * 2, input_dim) for final skip
   - Zero-init output_proj weights (std=0.01 or zero)
   - forward(x, t):
     - Time embedding via timestep_embedding from mlp.py
     - Encoder with skip storage
     - Decoder with skip concatenation
     - Final output with input skip

Import timestep_embedding from study.flow_matching.models.mlp (do NOT duplicate).
Follow the pattern from 06-RESEARCH.md Pattern 2.
  </action>
  <verify>
python -c "
from study.flow_matching.models.unet_mlp import UNetMLP, FiLMLayer
import torch

# Test FiLMLayer
film = FiLMLayer(256, 128)
x = torch.randn(4, 256)
c = torch.randn(4, 128)
out = film(x, c)
assert out.shape == (4, 256), f'FiLM shape wrong: {out.shape}'

# Test UNetMLP
model = UNetMLP()
params = sum(p.numel() for p in model.parameters())
print(f'UNetMLP params: {params:,}')

x = torch.randn(4, 1024)
t = torch.rand(4)
v = model(x, t)
assert v.shape == (4, 1024), f'Output shape wrong: {v.shape}'
assert not torch.isnan(v).any(), 'NaN in output'
print('UNetMLP test passed!')
"
  </verify>
  <done>UNetMLP and FiLMLayer implemented with correct shapes and FiLM identity initialization</done>
</task>

<task type="auto">
  <name>Task 2: Integrate UNetMLP into model factory</name>
  <files>study/flow_matching/models/__init__.py</files>
  <action>
Update study/flow_matching/models/__init__.py to:

1. Add import: from study.flow_matching.models.unet_mlp import UNetMLP, FiLMLayer
2. Add to __all__: "UNetMLP", "FiLMLayer"
3. Extend create_model() to handle arch="unet":
   - Default config: input_dim=1024, hidden_dims=(512, 256), time_embed_dim=256
   - Pass hidden_dims as tuple from kwargs.get()
4. Update docstring to document "unet" architecture option
5. Update error message available list to include "unet"
  </action>
  <verify>
python -c "
from study.flow_matching.models import create_model, UNetMLP, FiLMLayer

# Test factory
model = create_model('unet')
params = sum(p.numel() for p in model.parameters())
print(f'create_model(unet) params: {params:,}')
assert isinstance(model, UNetMLP)

# Test with custom config
model2 = create_model('unet', hidden_dims=(256, 128))
params2 = sum(p.numel() for p in model2.parameters())
print(f'create_model(unet, small) params: {params2:,}')
assert params2 < params

print('Factory integration test passed!')
"
  </verify>
  <done>create_model('unet') returns UNetMLP instance with correct default configuration</done>
</task>

<task type="auto">
  <name>Task 3: Verify training compatibility</name>
  <files>None (verification only)</files>
  <action>
Run a minimal training sanity check to verify UNetMLP works with FlowTrainer:

1. Create UNetMLP model
2. Load small dataset (1k split)
3. Run 5 training steps
4. Verify loss decreases and no NaN

Use CUDA_VISIBLE_DEVICES=1 per project guidelines.
  </action>
  <verify>
CUDA_VISIBLE_DEVICES=1 python -c "
import torch
from study.flow_matching.models import create_model
from study.flow_matching.data import SonarEmbeddingDataset
from study.flow_matching.coupling import create_coupling

# Setup
device = torch.device('cuda')
model = create_model('unet').to(device)
coupling = create_coupling('icfm')

# Load data
dataset = SonarEmbeddingDataset('study/datasets/vs_10k_normalized/1k/train.pt')
loader = torch.utils.data.DataLoader(dataset, batch_size=64, shuffle=True)

# Optimizer
optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)

# Train 5 steps
model.train()
losses = []
for i, batch in enumerate(loader):
    if i >= 5:
        break
    x1 = batch.to(device)
    x0 = torch.randn_like(x1)
    t = torch.rand(x1.shape[0], device=device)

    x_t, target = coupling(x0, x1, t)
    pred = model(x_t, t)
    loss = ((pred - target) ** 2).mean()

    optimizer.zero_grad()
    loss.backward()
    optimizer.step()

    losses.append(loss.item())
    print(f'Step {i+1}: loss={loss.item():.4f}')

# Verify
assert all(l < 100 for l in losses), 'Loss too high'
assert not any(torch.isnan(torch.tensor([l])) for l in losses), 'NaN in loss'
print(f'Training sanity check passed! Final loss: {losses[-1]:.4f}')
"
  </verify>
  <done>UNetMLP trains without NaN loss and loss decreases over 5 steps</done>
</task>

</tasks>

<verification>
All verification commands pass:
1. UNetMLP imports and runs forward pass correctly
2. FiLM layer initialized to identity (gamma=1, beta=0)
3. Factory returns UNetMLP for arch='unet'
4. 5-step training runs without NaN
</verification>

<success_criteria>
- UNetMLP class exists at study/flow_matching/models/unet_mlp.py
- FiLM conditioning starts as identity transform
- create_model('unet') returns working UNetMLP
- Model has ~2.5M parameters with default config
- Training loop runs without NaN loss
</success_criteria>

<output>
After completion, create `.planning/phases/06-advanced-architectures/06-01-SUMMARY.md`
</output>
