---
phase: 06-advanced-architectures
plan: 02
type: execute
wave: 1
depends_on: []
files_modified:
  - study/flow_matching/models/mamba_velocity.py
  - study/flow_matching/models/__init__.py
autonomous: true

must_haves:
  truths:
    - "Mamba velocity network installs and imports (if mamba-ssm available)"
    - "Bidirectional Mamba processes embedding as chunked sequence"
    - "Model produces valid velocity predictions with shape [B, 1024]"
    - "Graceful fallback if mamba-ssm not available"
  artifacts:
    - path: "study/flow_matching/models/mamba_velocity.py"
      provides: "MambaVelocityNetwork class (experimental)"
      exports: ["MambaVelocityNetwork", "MAMBA_AVAILABLE"]
    - path: "study/flow_matching/models/__init__.py"
      provides: "Extended factory with mamba support"
      contains: "mamba"
  key_links:
    - from: "study/flow_matching/models/mamba_velocity.py"
      to: "study/flow_matching/models/mlp.py"
      via: "import timestep_embedding"
      pattern: "from.*mlp import timestep_embedding"
    - from: "study/flow_matching/models/__init__.py"
      to: "study/flow_matching/models/mamba_velocity.py"
      via: "conditional import"
      pattern: "MambaVelocityNetwork"
---

<objective>
Implement experimental Mamba velocity network treating 1024-dim embedding as virtual sequence.

Purpose: Explore whether selective state-space models (SSMs) can learn velocity fields for flow matching. This is EXPERIMENTAL - Mamba is designed for sequences, not unordered embeddings. Document results either way as valid research.

Output: MambaVelocityNetwork class (~2M params) with graceful fallback if mamba-ssm unavailable.
</objective>

<execution_context>
@/home/prusek/.claude/get-shit-done/workflows/execute-plan.md
@/home/prusek/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/06-advanced-architectures/06-RESEARCH.md

Reference files:
@study/flow_matching/models/__init__.py
@study/flow_matching/models/mlp.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Install mamba-ssm dependency</name>
  <files>None (dependency installation)</files>
  <action>
Attempt to install mamba-ssm. This may fail due to CUDA version requirements.

1. First check CUDA version compatibility:
   python -c "import torch; print(f'PyTorch CUDA: {torch.version.cuda}')"
   nvcc --version (if available)

2. Install mamba-ssm:
   uv pip install mamba-ssm

3. If installation fails:
   - Log the error
   - Continue with plan - implementation will use try/except
   - Mark MAMBA_AVAILABLE = False in the module

Note: Installation failure is acceptable - we'll implement with graceful fallback.
  </action>
  <verify>
python -c "
try:
    import mamba_ssm
    print(f'mamba-ssm installed: version {mamba_ssm.__version__}')
    print('MAMBA_AVAILABLE = True')
except ImportError as e:
    print(f'mamba-ssm not available: {e}')
    print('MAMBA_AVAILABLE = False (will use fallback)')
"
  </verify>
  <done>mamba-ssm installation attempted, availability status known</done>
</task>

<task type="auto">
  <name>Task 2: Implement MambaVelocityNetwork</name>
  <files>study/flow_matching/models/mamba_velocity.py</files>
  <action>
Create study/flow_matching/models/mamba_velocity.py with:

1. **Try-except import for mamba-ssm:**
   ```python
   try:
       from mamba_ssm import Mamba
       MAMBA_AVAILABLE = True
   except ImportError:
       MAMBA_AVAILABLE = False
       Mamba = None
   ```

2. **MambaVelocityNetwork class** (~2M params with defaults):
   - Check MAMBA_AVAILABLE in __init__, raise ImportError if not
   - input_dim=1024, hidden_dim=256, d_state=16, d_conv=4, expand=2
   - num_layers=4, time_embed_dim=256, chunk_size=64 (16 chunks)
   - Verify 1024 % chunk_size == 0 in __init__

   Architecture:
   - time_mlp: Linear(time_embed_dim, hidden_dim) -> SiLU -> Linear
   - input_proj: Linear(chunk_size, hidden_dim)
   - mamba_fwd: ModuleList of Mamba layers (num_layers)
   - mamba_bwd: ModuleList of Mamba layers (num_layers) for bidirectional
   - time_projs: ModuleList of Linear(hidden_dim, hidden_dim) for time conditioning
   - output_proj: Linear(hidden_dim * 2, chunk_size) - zero-init

   forward(x, t):
   - Reshape [B, 1024] -> [B, 16, 64] (chunk)
   - Project to hidden: [B, 16, hidden_dim]
   - Forward Mamba on h_fwd
   - Backward Mamba on h_bwd = h.flip(1)
   - Add time conditioning after each layer
   - Concat forward and backward outputs
   - Project back: [B, 16, chunk_size] -> [B, 1024]

Import timestep_embedding from mlp.py (do NOT duplicate).
Follow the pattern from 06-RESEARCH.md Pattern 3.
  </action>
  <verify>
python -c "
from study.flow_matching.models.mamba_velocity import MAMBA_AVAILABLE

if MAMBA_AVAILABLE:
    from study.flow_matching.models.mamba_velocity import MambaVelocityNetwork
    import torch

    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    model = MambaVelocityNetwork().to(device)
    params = sum(p.numel() for p in model.parameters())
    print(f'MambaVelocityNetwork params: {params:,}')

    x = torch.randn(4, 1024, device=device)
    t = torch.rand(4, device=device)
    v = model(x, t)
    assert v.shape == (4, 1024), f'Output shape wrong: {v.shape}'
    assert not torch.isnan(v).any(), 'NaN in output'
    print('MambaVelocityNetwork test passed!')
else:
    print('Mamba not available - graceful fallback working')
    # Verify the class exists but raises on init
    try:
        from study.flow_matching.models.mamba_velocity import MambaVelocityNetwork
        MambaVelocityNetwork()
    except ImportError as e:
        print(f'Expected ImportError: {e}')
        print('Fallback behavior correct!')
"
  </verify>
  <done>MambaVelocityNetwork implemented with bidirectional processing and graceful fallback</done>
</task>

<task type="auto">
  <name>Task 3: Integrate Mamba into model factory</name>
  <files>study/flow_matching/models/__init__.py</files>
  <action>
Update study/flow_matching/models/__init__.py to:

1. Add conditional import:
   ```python
   from study.flow_matching.models.mamba_velocity import MambaVelocityNetwork, MAMBA_AVAILABLE
   ```

2. Add to __all__: "MambaVelocityNetwork", "MAMBA_AVAILABLE"

3. Extend create_model() for arch="mamba":
   - Check MAMBA_AVAILABLE first
   - If not available, raise ImportError with install instructions
   - Default config: input_dim=1024, hidden_dim=256, num_layers=4, d_state=16, chunk_size=64

4. Update docstring to document "mamba" (experimental)

5. Update error message available list:
   - List available architectures dynamically based on MAMBA_AVAILABLE
  </action>
  <verify>
python -c "
from study.flow_matching.models import MAMBA_AVAILABLE

if MAMBA_AVAILABLE:
    from study.flow_matching.models import create_model, MambaVelocityNetwork
    import torch

    device = torch.device('cuda')
    model = create_model('mamba').to(device)
    params = sum(p.numel() for p in model.parameters())
    print(f'create_model(mamba) params: {params:,}')
    assert isinstance(model, MambaVelocityNetwork)

    # Quick forward test
    x = torch.randn(4, 1024, device=device)
    t = torch.rand(4, device=device)
    v = model(x, t)
    print(f'Forward pass: {v.shape}')
    print('Factory integration test passed!')
else:
    print('Mamba not available - testing fallback')
    try:
        from study.flow_matching.models import create_model
        create_model('mamba')
    except ImportError as e:
        print(f'Expected ImportError: {e}')
        print('Fallback in factory works correctly!')
"
  </verify>
  <done>create_model('mamba') returns MambaVelocityNetwork or raises clear ImportError</done>
</task>

</tasks>

<verification>
All verification commands pass:
1. mamba-ssm installation status known
2. MambaVelocityNetwork imports (if available) or raises clear error (if not)
3. Bidirectional processing with chunk_size=64 works
4. Factory handles both available and unavailable cases
</verification>

<success_criteria>
- MambaVelocityNetwork class exists at study/flow_matching/models/mamba_velocity.py
- MAMBA_AVAILABLE flag correctly indicates availability
- create_model('mamba') works if available, raises clear error if not
- Bidirectional Mamba processes embedding as 16 chunks of 64 dims
- Model has ~2M parameters with default config (if available)
</success_criteria>

<output>
After completion, create `.planning/phases/06-advanced-architectures/06-02-SUMMARY.md`
</output>
