---
phase: 06-advanced-architectures
plan: 03
type: execute
wave: 2
depends_on:
  - "06-01"
  - "06-02"
files_modified:
  - study/flow_matching/models/scaling.py
  - study/flow_matching/models/__init__.py
autonomous: true

must_haves:
  truths:
    - "Tiny/Small/Base configs exist for all architecture types"
    - "get_scaled_config() returns valid config dicts"
    - "create_model() accepts scale parameter"
    - "Parameter counts follow ~4x jumps between scale levels"
  artifacts:
    - path: "study/flow_matching/models/scaling.py"
      provides: "SCALING_CONFIGS dict and get_scaled_config function"
      exports: ["SCALING_CONFIGS", "get_scaled_config"]
    - path: "study/flow_matching/models/__init__.py"
      provides: "Extended factory with scale parameter"
      contains: "scale"
  key_links:
    - from: "study/flow_matching/models/__init__.py"
      to: "study/flow_matching/models/scaling.py"
      via: "import scaling configs"
      pattern: "from.*scaling import"
---

<objective>
Implement architecture scaling variants (Tiny/Small/Base) for all velocity networks.

Purpose: Enable ablation studies measuring how model capacity interacts with dataset size. Following DiT-style scaling with ~4x parameter jumps between levels: Tiny (~500K), Small (~2M), Base (~10M).

Output: SCALING_CONFIGS dict, get_scaled_config() function, and extended create_model() supporting scale parameter.
</objective>

<execution_context>
@/home/prusek/.claude/get-shit-done/workflows/execute-plan.md
@/home/prusek/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/06-advanced-architectures/06-RESEARCH.md
@.planning/phases/06-advanced-architectures/06-01-SUMMARY.md
@.planning/phases/06-advanced-architectures/06-02-SUMMARY.md

Reference files:
@study/flow_matching/models/__init__.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Implement scaling configuration module</name>
  <files>study/flow_matching/models/scaling.py</files>
  <action>
Create study/flow_matching/models/scaling.py with:

1. **SCALING_CONFIGS dict** - Architecture configs for each scale:
   ```python
   SCALING_CONFIGS = {
       # MLP scaling: ~250K, ~1M, ~2.2M params
       "mlp_tiny": {"hidden_dim": 128, "num_layers": 4},
       "mlp_small": {"hidden_dim": 256, "num_layers": 5},  # Current default
       "mlp_base": {"hidden_dim": 384, "num_layers": 6},

       # DiT scaling: ~3M, ~9M, ~20M params
       "dit_tiny": {"hidden_dim": 256, "num_layers": 2, "num_heads": 4},
       "dit_small": {"hidden_dim": 384, "num_layers": 3, "num_heads": 6},  # Current default
       "dit_base": {"hidden_dim": 512, "num_layers": 4, "num_heads": 8},

       # U-Net MLP scaling: ~600K, ~2.5M, ~5.5M params
       "unet_tiny": {"hidden_dims": (256, 128)},
       "unet_small": {"hidden_dims": (512, 256)},  # Default
       "unet_base": {"hidden_dims": (768, 384)},

       # Mamba scaling (experimental): ~500K, ~2M, ~5M params
       "mamba_tiny": {"hidden_dim": 128, "num_layers": 2},
       "mamba_small": {"hidden_dim": 256, "num_layers": 4},  # Default
       "mamba_base": {"hidden_dim": 384, "num_layers": 6},
   }
   ```

2. **get_scaled_config(arch, scale)** function:
   - Accepts arch: str ('mlp', 'dit', 'unet', 'mamba')
   - Accepts scale: str ('tiny', 'small', 'base')
   - Constructs key as f"{arch}_{scale}"
   - Returns config dict copy
   - Raises ValueError with available configs if key not found

3. **list_available_scales()** function:
   - Returns dict mapping arch to list of available scales
   - Useful for CLI help text

Add docstrings explaining the scaling rationale (~4x param jumps).
  </action>
  <verify>
python -c "
from study.flow_matching.models.scaling import SCALING_CONFIGS, get_scaled_config, list_available_scales

# Test get_scaled_config
mlp_tiny = get_scaled_config('mlp', 'tiny')
assert mlp_tiny['hidden_dim'] == 128
print(f'mlp_tiny: {mlp_tiny}')

unet_base = get_scaled_config('unet', 'base')
assert unet_base['hidden_dims'] == (768, 384)
print(f'unet_base: {unet_base}')

# Test error handling
try:
    get_scaled_config('invalid', 'tiny')
except ValueError as e:
    print(f'Expected error: {e}')

# Test list_available_scales
scales = list_available_scales()
print(f'Available scales: {scales}')
assert 'mlp' in scales
assert 'tiny' in scales['mlp']
print('Scaling module tests passed!')
"
  </verify>
  <done>SCALING_CONFIGS and get_scaled_config() work correctly</done>
</task>

<task type="auto">
  <name>Task 2: Extend model factory with scale parameter</name>
  <files>study/flow_matching/models/__init__.py</files>
  <action>
Update study/flow_matching/models/__init__.py to:

1. Add imports:
   ```python
   from study.flow_matching.models.scaling import SCALING_CONFIGS, get_scaled_config, list_available_scales
   ```

2. Add to __all__: "SCALING_CONFIGS", "get_scaled_config", "list_available_scales"

3. Modify create_model() signature:
   ```python
   def create_model(arch: str, scale: str = None, **kwargs) -> torch.nn.Module:
   ```

4. Update create_model() logic:
   - If scale is provided, get base config from get_scaled_config(arch, scale)
   - Merge kwargs to allow overrides: config.update(kwargs)
   - Use merged config for model creation
   - If scale is None, use current defaults (backward compatible)

5. Update docstring to document scale parameter with examples:
   - create_model('mlp') - uses default config
   - create_model('mlp', 'tiny') - uses tiny config
   - create_model('mlp', 'base', hidden_dim=512) - base config with override

6. Log warning if scale conflicts with explicit kwargs (e.g., scale='tiny' but hidden_dim=512)
  </action>
  <verify>
python -c "
from study.flow_matching.models import create_model, SCALING_CONFIGS, get_scaled_config
import torch

# Test scale parameter
model_tiny = create_model('mlp', 'tiny')
model_base = create_model('mlp', 'base')
tiny_params = sum(p.numel() for p in model_tiny.parameters())
base_params = sum(p.numel() for p in model_base.parameters())
print(f'mlp_tiny: {tiny_params:,} params')
print(f'mlp_base: {base_params:,} params')
assert base_params > tiny_params * 3, 'Base should be ~4x larger than tiny'

# Test backward compatibility (no scale)
model_default = create_model('mlp')
default_params = sum(p.numel() for p in model_default.parameters())
print(f'mlp_default: {default_params:,} params')

# Test override
model_override = create_model('mlp', 'tiny', hidden_dim=512)
override_params = sum(p.numel() for p in model_override.parameters())
print(f'mlp_tiny_override: {override_params:,} params')
assert override_params > tiny_params, 'Override should increase params'

# Test UNet scaling
unet_tiny = create_model('unet', 'tiny')
unet_base = create_model('unet', 'base')
unet_tiny_params = sum(p.numel() for p in unet_tiny.parameters())
unet_base_params = sum(p.numel() for p in unet_base.parameters())
print(f'unet_tiny: {unet_tiny_params:,} params')
print(f'unet_base: {unet_base_params:,} params')

print('Factory scale tests passed!')
"
  </verify>
  <done>create_model() accepts scale parameter and applies scaling configs correctly</done>
</task>

<task type="auto">
  <name>Task 3: Verify all architectures at all scales</name>
  <files>None (verification only)</files>
  <action>
Create and verify all architecture/scale combinations:

1. For each arch in [mlp, dit, unet]:
   - Create tiny, small, base variants
   - Count parameters
   - Run forward pass
   - Verify no NaN

2. For mamba (if available):
   - Same verification if MAMBA_AVAILABLE
   - Skip gracefully if not

3. Print parameter count table matching 06-RESEARCH.md targets.

Use CUDA_VISIBLE_DEVICES=1 for GPU tests.
  </action>
  <verify>
CUDA_VISIBLE_DEVICES=1 python -c "
import torch
from study.flow_matching.models import create_model, MAMBA_AVAILABLE

device = torch.device('cuda')
results = []

architectures = ['mlp', 'dit', 'unet']
if MAMBA_AVAILABLE:
    architectures.append('mamba')
else:
    print('Note: Mamba not available, skipping mamba tests')

scales = ['tiny', 'small', 'base']

for arch in architectures:
    for scale in scales:
        model = create_model(arch, scale).to(device)
        params = sum(p.numel() for p in model.parameters())

        # Forward pass test
        x = torch.randn(4, 1024, device=device)
        t = torch.rand(4, device=device)
        v = model(x, t)

        assert v.shape == (4, 1024), f'{arch}_{scale}: wrong output shape'
        assert not torch.isnan(v).any(), f'{arch}_{scale}: NaN in output'

        results.append((arch, scale, params))
        print(f'{arch}_{scale}: {params:>12,} params - OK')

# Print summary table
print()
print('=' * 50)
print('Architecture Scaling Summary')
print('=' * 50)
print(f'{\"Arch\":<8} {\"Tiny\":>12} {\"Small\":>12} {\"Base\":>12}')
print('-' * 50)

for arch in architectures:
    arch_results = [r for r in results if r[0] == arch]
    tiny = next((r[2] for r in arch_results if r[1] == 'tiny'), 0)
    small = next((r[2] for r in arch_results if r[1] == 'small'), 0)
    base = next((r[2] for r in arch_results if r[1] == 'base'), 0)
    print(f'{arch:<8} {tiny:>12,} {small:>12,} {base:>12,}')

print('=' * 50)
print('All architecture/scale combinations verified!')
"
  </verify>
  <done>All architecture/scale combinations create valid models with expected parameter counts</done>
</task>

</tasks>

<verification>
All verification commands pass:
1. SCALING_CONFIGS contains all arch/scale combinations
2. get_scaled_config() returns valid configs
3. create_model() with scale parameter works for all architectures
4. Parameter counts follow ~4x jumps between levels
5. All models produce valid forward passes
</verification>

<success_criteria>
- scaling.py module exists with SCALING_CONFIGS and get_scaled_config()
- create_model(arch, scale) works for all supported combinations
- Backward compatibility preserved (scale=None uses defaults)
- Parameter counts match targets from 06-RESEARCH.md within reasonable margin
- All architectures produce valid [B, 1024] velocity outputs
</success_criteria>

<output>
After completion, create `.planning/phases/06-advanced-architectures/06-03-SUMMARY.md`
</output>
