---
phase: 01-data-pipeline
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - study/data/generate_dataset.py
  - study/data/create_splits.py
  - study/datasets/vs_10k.pt
  - study/datasets/splits/1k/train.pt
  - study/datasets/splits/1k/val.pt
  - study/datasets/splits/1k/test.pt
  - study/datasets/splits/5k/train.pt
  - study/datasets/splits/5k/val.pt
  - study/datasets/splits/5k/test.pt
  - study/datasets/splits/10k/train.pt
  - study/datasets/splits/10k/val.pt
  - study/datasets/splits/10k/test.pt
autonomous: true

must_haves:
  truths:
    - "10K VS dataset exists with valid SONAR embeddings (1024D)"
    - "Nested splits exist where 1K is subset of 5K is subset of 10K"
    - "Train/val/test ratio is 80/10/10 for all split sizes"
    - "All embeddings are float32 and unnormalized"
  artifacts:
    - path: "study/datasets/vs_10k.pt"
      provides: "Full 10K dataset with embeddings, instructions, metadata"
      contains: "embeddings.*torch.Size.*10000.*1024"
    - path: "study/datasets/splits/10k/train.pt"
      provides: "10K training split (8000 samples)"
      min_lines: 1
    - path: "study/datasets/splits/5k/train.pt"
      provides: "5K training split (4000 samples)"
      min_lines: 1
    - path: "study/datasets/splits/1k/train.pt"
      provides: "1K training split (800 samples)"
      min_lines: 1
  key_links:
    - from: "study/data/create_splits.py"
      to: "study/datasets/vs_10k.pt"
      via: "torch.load to read full dataset"
      pattern: "torch\\.load.*vs_10k"
---

<objective>
Generate a 10K verbosed sampling (VS) dataset with SONAR embeddings and create nested train/val/test splits for 1K, 5K, and 10K dataset sizes.

Purpose: Establish the data foundation for flow matching architecture experiments. The nested split structure ensures fair comparisons when ablating dataset size.

Output: Full 10K dataset file and 9 split files (3 sizes x 3 splits) in the study directory.
</objective>

<execution_context>
@/home/prusek/.claude/get-shit-done/workflows/execute-plan.md
@/home/prusek/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/01-data-pipeline/01-CONTEXT.md
@.planning/phases/01-data-pipeline/01-RESEARCH.md
@ecoflow/decoder.py
@datasets/gsm8k_instructions_vs.pt (reference format)
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create VS dataset generation script</name>
  <files>study/data/generate_dataset.py</files>
  <action>
Create a dataset generation script that produces 10K verbosed sampling instructions with SONAR embeddings.

**Key requirements:**
1. Use the LLM client from `shared/llm_client.py` to generate instructions via VS prompting
2. Use SONAR encoder from `sonar.inference_pipelines.text.TextToEmbeddingModelPipeline` with:
   - `encoder="text_sonar_basic_encoder"`
   - `tokenizer="text_sonar_basic_encoder"`
   - `device=torch.device("cuda:0")`
   - `dtype=torch.float16` for encoding speed
3. Convert embeddings to float32 before storage (avoid fp16 precision loss)
4. Store in same format as `datasets/gsm8k_instructions_vs.pt`:
   ```python
   {
       "embeddings": torch.Tensor,  # [10000, 1024], float32
       "instructions": list[str],   # 10000 text prompts
       "sources": {"verbalized_sampling": 10000},
       "config": {...},  # Generation config
       "stats": {"n_text_instructions": 10000, "total": 10000, "embedding_dim": 1024}
   }
   ```
5. Batch encoding with batch_size=256 (safe for GPU memory)
6. Progress bar with tqdm
7. Set random seed=42 for reproducibility
8. Use GSM8K questions as seed problems for VS generation
9. Save to `study/datasets/vs_10k.pt`

**VS prompting approach:**
- Sample 3-5 GSM8K problems
- Ask LLM to generate meta-cognitive instructions about how to solve such problems
- Deduplicate by semantic similarity (threshold=0.95) during generation

The script should be runnable as: `uv run python study/data/generate_dataset.py`

Note: If 10K generation is slow, implement checkpointing to save progress every 1000 samples.
  </action>
  <verify>
Run: `uv run python study/data/generate_dataset.py --dry-run` (implement dry-run flag that generates 100 samples to verify pipeline works)
Check: Script completes without errors, outputs sample embeddings with correct shape [100, 1024]
  </verify>
  <done>
- Script exists at `study/data/generate_dataset.py`
- Dry-run generates valid embeddings
- Config saved matches generation parameters
  </done>
</task>

<task type="auto">
  <name>Task 2: Create split generation script and generate all splits</name>
  <files>
study/data/create_splits.py
study/datasets/splits/1k/train.pt
study/datasets/splits/1k/val.pt
study/datasets/splits/1k/test.pt
study/datasets/splits/5k/train.pt
study/datasets/splits/5k/val.pt
study/datasets/splits/5k/test.pt
study/datasets/splits/10k/train.pt
study/datasets/splits/10k/val.pt
study/datasets/splits/10k/test.pt
  </files>
  <action>
Create a split generation script that creates nested train/val/test splits for 1K, 5K, and 10K dataset sizes.

**Key requirements:**
1. Load full dataset from `study/datasets/vs_10k.pt`
2. Single shuffle with fixed seed=42 using `np.random.permutation`
3. Nested structure: 1K indices are first 1000 of shuffled, 5K are first 5000, 10K are all 10000
4. Split ratio: 80% train, 10% val, 10% test
   - 1K: 800 train, 100 val, 100 test
   - 5K: 4000 train, 500 val, 500 test
   - 10K: 8000 train, 1000 val, 1000 test
5. Each split file contains:
   ```python
   {
       "embeddings": torch.Tensor,  # [N, 1024], float32
       "instructions": list[str],   # N text prompts
       "indices": list[int],        # Original indices in vs_10k.pt
       "split": str,                # "train", "val", or "test"
       "size": str,                 # "1k", "5k", or "10k"
   }
   ```
6. Verify nested property: 1K train indices subset of 5K train indices subset of 10K train indices

**Directory structure:**
```
study/datasets/splits/
├── 1k/
│   ├── train.pt
│   ├── val.pt
│   └── test.pt
├── 5k/
│   ├── train.pt
│   ├── val.pt
│   └── test.pt
└── 10k/
    ├── train.pt
    ├── val.pt
    └── test.pt
```

Since the full dataset doesn't exist yet (Task 1 just creates the generation script), this task should:
1. First run the full dataset generation: `uv run python study/data/generate_dataset.py`
2. Then create and run the split script

Note: If full generation takes too long (>2 hours), consider:
- Using existing `datasets/gsm8k_instructions_vs.pt` (4070 samples) as a temporary base
- Generating only the additional ~6000 samples needed
- Or proceeding with 4K subset for initial testing
  </action>
  <verify>
Run: `uv run python study/data/create_splits.py --verify`
Checks:
1. All 9 split files exist
2. Sample counts match expected (800/100/100 for 1K, etc.)
3. Nested property holds: set(1k_train_indices).issubset(set(5k_train_indices))
4. No data leakage: train/val/test indices are disjoint within each size
  </verify>
  <done>
- 9 split files exist in `study/datasets/splits/`
- Nested property verified
- No overlapping samples between train/val/test
- Embeddings are float32 and shape [N, 1024]
  </done>
</task>

</tasks>

<verification>
After both tasks complete:

1. **Dataset existence check:**
   ```bash
   ls -la study/datasets/vs_10k.pt
   ls -la study/datasets/splits/*/
   ```

2. **Dataset integrity check:**
   ```python
   import torch
   d = torch.load("study/datasets/vs_10k.pt")
   assert d["embeddings"].shape == torch.Size([10000, 1024])
   assert d["embeddings"].dtype == torch.float32
   assert len(d["instructions"]) == 10000
   ```

3. **Split integrity check:**
   ```python
   train_1k = torch.load("study/datasets/splits/1k/train.pt")
   train_5k = torch.load("study/datasets/splits/5k/train.pt")
   assert len(train_1k["embeddings"]) == 800
   assert len(train_5k["embeddings"]) == 4000
   assert set(train_1k["indices"]).issubset(set(train_5k["indices"]))
   ```

4. **No data leakage:**
   ```python
   for size in ["1k", "5k", "10k"]:
       train = torch.load(f"study/datasets/splits/{size}/train.pt")
       val = torch.load(f"study/datasets/splits/{size}/val.pt")
       test = torch.load(f"study/datasets/splits/{size}/test.pt")
       assert len(set(train["indices"]) & set(val["indices"])) == 0
       assert len(set(train["indices"]) & set(test["indices"])) == 0
       assert len(set(val["indices"]) & set(test["indices"])) == 0
   ```
</verification>

<success_criteria>
- [ ] `study/datasets/vs_10k.pt` exists with 10000 samples, 1024D embeddings, float32
- [ ] All 9 split files exist with correct sample counts
- [ ] Nested property verified: 1K train subset of 5K train subset of 10K train
- [ ] No data leakage between train/val/test splits
- [ ] All generation scripts are runnable and documented
</success_criteria>

<output>
After completion, create `.planning/phases/01-data-pipeline/01-01-SUMMARY.md`
</output>
