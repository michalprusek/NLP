---
phase: 01-data-pipeline
plan: 02
type: execute
wave: 2
depends_on: ["01-01"]
files_modified:
  - study/data/normalize.py
  - study/data/verify_decoder.py
  - study/data/dataset.py
  - study/datasets/normalization_stats.pt
autonomous: true

must_haves:
  truths:
    - "Normalization statistics computed from 10K training set only"
    - "Denormalized embeddings produce coherent text via SONAR decoder"
    - "Round-trip cosine similarity >= 0.9 for 95%+ of samples"
    - "Data loading pipeline returns properly normalized tensors"
  artifacts:
    - path: "study/datasets/normalization_stats.pt"
      provides: "Per-dimension mean and std tensors for z-score normalization"
      contains: "mean.*std"
    - path: "study/data/normalize.py"
      provides: "normalize() and denormalize() functions"
      exports: ["normalize", "denormalize", "compute_stats", "load_stats"]
    - path: "study/data/verify_decoder.py"
      provides: "Round-trip verification script with cosine similarity"
      min_lines: 50
    - path: "study/data/dataset.py"
      provides: "FlowDataset class for training data loading"
      contains: "class FlowDataset"
  key_links:
    - from: "study/data/normalize.py"
      to: "study/datasets/normalization_stats.pt"
      via: "torch.save to store stats"
      pattern: "torch\\.save.*normalization_stats"
    - from: "study/data/verify_decoder.py"
      to: "study/data/normalize.py"
      via: "import denormalize"
      pattern: "from.*normalize.*import.*denormalize"
    - from: "study/data/dataset.py"
      to: "study/data/normalize.py"
      via: "import normalize for data loading"
      pattern: "from.*normalize.*import"
---

<objective>
Implement per-dimension normalization with stored statistics and verify SONAR decoder produces coherent text from denormalized embeddings.

Purpose: Flow models train better on normalized data (zero mean, unit variance). The verification ensures the pipeline preserves semantic content through the normalize-denormalize cycle.

Output: Normalization utilities, statistics file, verification script, and data loading pipeline.
</objective>

<execution_context>
@/home/prusek/.claude/get-shit-done/workflows/execute-plan.md
@/home/prusek/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/01-data-pipeline/01-CONTEXT.md
@.planning/phases/01-data-pipeline/01-RESEARCH.md
@.planning/phases/01-data-pipeline/01-01-SUMMARY.md
@ecoflow/decoder.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Implement normalization utilities and compute statistics</name>
  <files>
study/data/normalize.py
study/datasets/normalization_stats.pt
  </files>
  <action>
Create normalization utilities with per-dimension z-score normalization.

**Key requirements:**
1. Compute mean and std per dimension (1024 values each) from 10K TRAINING SET ONLY
2. Add epsilon (1e-8) to std for numerical stability
3. Store stats as `study/datasets/normalization_stats.pt`:
   ```python
   {
       "mean": torch.Tensor,  # [1024], float32
       "std": torch.Tensor,   # [1024], float32
       "source": "10k_train", # Provenance tracking
       "n_samples": 8000,     # Number of samples used
   }
   ```
4. Implement functions:
   ```python
   def compute_stats(embeddings: torch.Tensor) -> dict:
       """Compute mean/std per dimension. Call ONLY on training data."""

   def save_stats(stats: dict, path: str) -> None:
       """Save normalization statistics to file."""

   def load_stats(path: str) -> dict:
       """Load normalization statistics from file."""

   def normalize(embeddings: torch.Tensor, stats: dict) -> torch.Tensor:
       """Apply z-score normalization: (x - mean) / std"""

   def denormalize(embeddings: torch.Tensor, stats: dict) -> torch.Tensor:
       """Reverse normalization: x * std + mean"""
   ```
5. Add assertion in denormalize to warn if embeddings look already denormalized (mean >> 0)

**CRITICAL: Compute stats from 10K training set ONLY (8000 samples), not val/test.**

The script should:
1. Load `study/datasets/splits/10k/train.pt`
2. Compute stats from its embeddings
3. Save to `study/datasets/normalization_stats.pt`

Make runnable as: `uv run python study/data/normalize.py --compute-stats`
  </action>
  <verify>
Run: `uv run python study/data/normalize.py --compute-stats --verify`
Checks:
1. Stats file exists with correct shape [1024] for mean and std
2. Round-trip: normalize(denormalize(x, stats), stats) == x (within float tolerance)
3. Normalized embeddings have ~0 mean and ~1 std per dimension
  </verify>
  <done>
- `study/data/normalize.py` exists with all 5 functions
- `study/datasets/normalization_stats.pt` exists with mean/std tensors
- Stats computed from 8000 training samples only
- Round-trip is numerically stable
  </done>
</task>

<task type="auto">
  <name>Task 2: Verify SONAR decoder with round-trip test</name>
  <files>study/data/verify_decoder.py</files>
  <action>
Create a verification script that tests SONAR decoder fidelity via round-trip encoding.

**Round-trip test:**
1. Take original text and its embedding from dataset
2. Denormalize the embedding (in case it was normalized)
3. Decode embedding to text via SONAR decoder
4. Re-encode decoded text to embedding
5. Compute cosine similarity between original and re-encoded embedding
6. Pass if similarity >= 0.9

**Key requirements:**
1. Use `SonarDecoder` from `ecoflow/decoder.py` for decoding
2. Use `TextToEmbeddingModelPipeline` for re-encoding
3. Test on a sample of embeddings (e.g., 500 from test set)
4. Report:
   - Mean cosine similarity
   - Pass rate (% samples with similarity >= 0.9)
   - List of failure cases (indices with similarity < 0.9)
5. Pipeline passes if 95%+ samples meet threshold
6. Log failure cases for manual review (save to `study/datasets/verification_failures.json`)

**Implementation:**
```python
def verify_round_trip(
    embeddings: torch.Tensor,
    texts: list[str],
    stats: dict,  # For denormalization
    threshold: float = 0.9,
    sample_size: int = 500,
) -> dict:
    """
    Verify SONAR decoder round-trip fidelity.

    Returns:
        {
            "mean_similarity": float,
            "pass_rate": float,
            "threshold": float,
            "n_tested": int,
            "failures": list[dict],  # {index, similarity, original_text, decoded_text}
        }
    """
```

Make runnable as: `uv run python study/data/verify_decoder.py`

**Device:** Use CUDA:0 for both encoder and decoder. Batch decode with batch_size=32.
  </action>
  <verify>
Run: `uv run python study/data/verify_decoder.py`
Checks:
1. Script completes without OOM or errors
2. Mean cosine similarity printed
3. Pass rate >= 95% (0.95)
4. Failure cases logged to JSON if any
  </verify>
  <done>
- Verification script exists and runs
- Pass rate >= 95% for cosine similarity >= 0.9
- Failures logged for review
- SONAR decoder confirmed working with denormalized embeddings
  </done>
</task>

<task type="auto">
  <name>Task 3: Create data loading pipeline</name>
  <files>study/data/dataset.py</files>
  <action>
Create a PyTorch Dataset class for loading normalized embeddings for flow model training.

**Key requirements:**
1. `FlowDataset` class that:
   - Loads split file (e.g., `study/datasets/splits/5k/train.pt`)
   - Loads normalization stats from `study/datasets/normalization_stats.pt`
   - Returns normalized embeddings on `__getitem__`
   - Stores original (unnormalized) embeddings for decoder use
2. Support for all split sizes (1k, 5k, 10k) and types (train, val, test)
3. Reproducible shuffling with seed parameter
4. DataLoader creation helper with:
   - `pin_memory=True`
   - `num_workers=4`
   - Configurable batch_size
   - Worker seed management for reproducibility

**Implementation:**
```python
class FlowDataset(torch.utils.data.Dataset):
    def __init__(
        self,
        split_path: str,
        stats_path: str = "study/datasets/normalization_stats.pt",
        return_normalized: bool = True,
    ):
        """
        Load embeddings with optional normalization.

        Args:
            split_path: Path to split file (e.g., "study/datasets/splits/5k/train.pt")
            stats_path: Path to normalization stats
            return_normalized: If True, __getitem__ returns normalized embeddings
        """

    def __len__(self) -> int:
        return len(self.embeddings)

    def __getitem__(self, idx: int) -> torch.Tensor:
        """Return (optionally normalized) embedding at index."""

    def get_original(self, idx: int) -> tuple[torch.Tensor, str]:
        """Return original embedding and instruction text for decoder."""

    @property
    def embedding_dim(self) -> int:
        return 1024


def create_dataloader(
    dataset: FlowDataset,
    batch_size: int = 256,
    shuffle: bool = True,
    seed: int = 42,
    num_workers: int = 4,
) -> torch.utils.data.DataLoader:
    """Create reproducible DataLoader with proper worker seeding."""
```

Add a quick test at bottom of file that loads each split and verifies shape.
  </action>
  <verify>
Run: `uv run python study/data/dataset.py`
Checks:
1. Loads all 9 splits without errors
2. Normalized embeddings have ~0 mean, ~1 std
3. DataLoader iteration works
4. Reproducibility: same seed produces same batch order
  </verify>
  <done>
- `FlowDataset` class implemented with normalization
- `create_dataloader` helper implemented
- All splits loadable
- Normalized data has correct statistics
  </done>
</task>

</tasks>

<verification>
After all tasks complete:

1. **Normalization stats check:**
   ```python
   import torch
   stats = torch.load("study/datasets/normalization_stats.pt")
   assert stats["mean"].shape == torch.Size([1024])
   assert stats["std"].shape == torch.Size([1024])
   assert stats["n_samples"] == 8000
   assert (stats["std"] > 0).all()  # No zero std
   ```

2. **Round-trip verification:**
   ```bash
   uv run python study/data/verify_decoder.py
   # Should report pass_rate >= 0.95
   ```

3. **Data loading pipeline:**
   ```python
   from study.data.dataset import FlowDataset, create_dataloader

   ds = FlowDataset("study/datasets/splits/5k/train.pt")
   loader = create_dataloader(ds, batch_size=256)
   batch = next(iter(loader))

   assert batch.shape == torch.Size([256, 1024])
   assert batch.mean().abs() < 0.1  # Approximately zero mean
   assert (batch.std() - 1.0).abs() < 0.3  # Approximately unit std
   ```

4. **Normalization round-trip:**
   ```python
   from study.data.normalize import normalize, denormalize, load_stats

   stats = load_stats("study/datasets/normalization_stats.pt")
   original = torch.randn(10, 1024)
   normalized = normalize(original, stats)
   recovered = denormalize(normalized, stats)

   assert torch.allclose(original, recovered, atol=1e-5)
   ```
</verification>

<success_criteria>
- [ ] `study/datasets/normalization_stats.pt` exists with mean/std from 10K train only
- [ ] `study/data/normalize.py` has normalize/denormalize functions that are inverses
- [ ] Round-trip verification passes with >= 95% samples having cosine similarity >= 0.9
- [ ] `FlowDataset` loads splits and returns normalized embeddings
- [ ] DataLoader creates reproducible batches with proper worker seeding
</success_criteria>

<output>
After completion, create `.planning/phases/01-data-pipeline/01-02-SUMMARY.md`
</output>
