---
phase: 03-baseline-architectures
plan: 03
type: execute
wave: 1
depends_on: []
files_modified:
  - study/flow_matching/evaluate.py
autonomous: true
gap_closure: true

must_haves:
  truths:
    - "Both architectures produce reasonable reconstruction MSE (<0.1)"
    - "Generated embeddings decode to coherent English text"
  artifacts:
    - path: "study/flow_matching/evaluate.py"
      provides: "Reconstruction MSE and text generation verification"
      exports: ["compute_reconstruction_mse", "generate_and_decode", "main"]
  key_links:
    - from: "evaluate.py"
      to: "models/__init__.py"
      via: "create_model() factory"
      pattern: "from study.flow_matching.models import create_model"
    - from: "evaluate.py"
      to: "rielbo/decoder.py"
      via: "SonarDecoder import"
      pattern: "from rielbo.decoder import SonarDecoder"
---

<objective>
Close Phase 3 verification gaps by implementing reconstruction MSE and text generation tests.

Purpose: The verifier found that Phase 3 claims "reconstruction MSE < 0.1" and "coherent text generation" but no actual measurements exist. This plan creates the evaluation infrastructure and runs it on trained checkpoints.

Output:
- `study/flow_matching/evaluate.py` with reconstruction MSE and text generation tests
- Actual MSE measurements for MLP and DiT checkpoints
- Decoded text samples from both architectures
</objective>

<execution_context>
@/home/prusek/.claude/get-shit-done/workflows/execute-plan.md
@/home/prusek/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/03-baseline-architectures/03-01-SUMMARY.md
@.planning/phases/03-baseline-architectures/03-02-SUMMARY.md
@.planning/phases/03-baseline-architectures/03-VERIFICATION.md

Key infrastructure files:
@study/flow_matching/models/__init__.py - create_model() factory
@study/flow_matching/trainer.py - FlowTrainer with ICFM formulation
@study/data/normalize.py - denormalize() function
@study/data/dataset.py - FlowDataset with test split loading
@rielbo/decoder.py - SonarDecoder for embedding-to-text

Trained checkpoints:
- study/checkpoints/mlp-icfm-1k-none/best.pt (MLP, 2 epochs)
- study/checkpoints/dit-icfm-1k-none/best.pt (DiT, 2 epochs)
- study/checkpoints/mlp-icfm-5k-none/best.pt (MLP, 3 epochs, extended)

Normalization stats:
- study/datasets/normalization_stats.pt
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create evaluate.py with reconstruction MSE and text generation</name>
  <files>study/flow_matching/evaluate.py</files>
  <action>
Create study/flow_matching/evaluate.py with two core functions:

1. **compute_reconstruction_mse(model, test_dataset, n_samples, n_steps, device)**
   - Takes normalized test embeddings x1 (target)
   - Samples random noise x0 ~ N(0, I)
   - Uses Euler ODE integration from t=0 to t=1 with n_steps steps:
     - For each step i: t = i/n_steps, x_t = x_t + (1/n_steps) * model(x_t, t)
     - Start from x0, end at x1_hat
   - Computes MSE = mean((x1 - x1_hat)^2)
   - Returns dict with mse, std, n_samples

2. **generate_and_decode(model, stats, decoder, n_samples, n_steps, device)**
   - Samples random noise x0 ~ N(0, I) of shape [n_samples, 1024]
   - Runs Euler ODE integration to get x1_hat (normalized)
   - Denormalizes x1_hat using stats (call denormalize from study.data.normalize)
   - Decodes using SonarDecoder.decode()
   - Returns list of decoded text strings

3. **main()** CLI with:
   - --checkpoint: Path to checkpoint (required)
   - --arch: Architecture name ('mlp' or 'dit')
   - --test-split: Path to test split (default: study/datasets/splits/1k/test.pt)
   - --stats-path: Path to normalization stats
   - --n-mse-samples: Samples for MSE (default: 100)
   - --n-gen-samples: Samples for generation (default: 5)
   - --n-steps: ODE integration steps (default: 100)
   - --device: CUDA device (default: cuda:0)

Checkpoint loading:
- Load with torch.load(path, weights_only=True)
- Extract model_state_dict key
- Create model with create_model(arch)
- Load state dict with model.load_state_dict()

Output format:
```
=== Reconstruction MSE ===
MSE: 0.0XXX +/- 0.0XXX
Samples: N
Steps: M

=== Generated Text Samples ===
[1] "..."
[2] "..."
...
```

Use @torch.no_grad() decorator for both functions.
Use tqdm for progress bars during ODE integration.
Set model.eval() before inference.
  </action>
  <verify>
Run: `uv run python -c "from study.flow_matching.evaluate import compute_reconstruction_mse, generate_and_decode; print('Imports OK')"`
Expected: "Imports OK" with no errors
  </verify>
  <done>evaluate.py exists with compute_reconstruction_mse and generate_and_decode functions that import correctly</done>
</task>

<task type="auto">
  <name>Task 2: Run evaluation on all trained checkpoints</name>
  <files>None (verification only)</files>
  <action>
Run evaluate.py on all three trained checkpoints and verify results:

1. **MLP 1K checkpoint:**
```bash
CUDA_VISIBLE_DEVICES=1 uv run python -m study.flow_matching.evaluate \
  --checkpoint study/checkpoints/mlp-icfm-1k-none/best.pt \
  --arch mlp \
  --test-split study/datasets/splits/1k/test.pt \
  --n-mse-samples 100 \
  --n-gen-samples 5 \
  --n-steps 100
```

2. **DiT 1K checkpoint:**
```bash
CUDA_VISIBLE_DEVICES=1 uv run python -m study.flow_matching.evaluate \
  --checkpoint study/checkpoints/dit-icfm-1k-none/best.pt \
  --arch dit \
  --test-split study/datasets/splits/1k/test.pt \
  --n-mse-samples 100 \
  --n-gen-samples 5 \
  --n-steps 100
```

3. **MLP 5K checkpoint (extended training):**
```bash
CUDA_VISIBLE_DEVICES=1 uv run python -m study.flow_matching.evaluate \
  --checkpoint study/checkpoints/mlp-icfm-5k-none/best.pt \
  --arch mlp \
  --test-split study/datasets/splits/5k/test.pt \
  --n-mse-samples 100 \
  --n-gen-samples 5 \
  --n-steps 100
```

Verification criteria:
- All MSE values should be < 0.1
- Generated text should be coherent English sentences
- Text should be semantically related to problem-solving/reasoning domain

Record actual MSE values and 3 sample texts for each checkpoint in SUMMARY.
  </action>
  <verify>
Run all three commands above successfully without errors.
Check MSE < 0.1 for all checkpoints.
Verify generated text is coherent English (manual inspection).
  </verify>
  <done>All three checkpoints pass MSE < 0.1 criterion and generate coherent English text</done>
</task>

</tasks>

<verification>
After completing both tasks:

1. **Reconstruction MSE verification:**
   - MLP 1K: MSE < 0.1
   - DiT 1K: MSE < 0.1
   - MLP 5K: MSE < 0.1

2. **Text generation verification:**
   - All decoded samples are coherent English
   - Semantic content relates to reasoning/problem-solving

3. **Code quality:**
   - evaluate.py follows project patterns
   - No TODO or placeholder code
   - Functions documented with docstrings
</verification>

<success_criteria>
1. `study/flow_matching/evaluate.py` exists and imports without error
2. All three checkpoints produce reconstruction MSE < 0.1
3. Generated text samples are coherent English sentences
4. SUMMARY documents actual MSE values and sample texts
</success_criteria>

<output>
After completion, create `.planning/phases/03-baseline-architectures/03-03-SUMMARY.md` with:
- Actual MSE measurements for all checkpoints
- 3-5 sample decoded texts from each architecture
- Verification that both Phase 3 success criteria are now satisfied
</output>
