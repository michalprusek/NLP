---
phase: 03-baseline-architectures
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - study/flow_matching/models/__init__.py
  - study/flow_matching/models/mlp.py
  - study/flow_matching/train.py
autonomous: true

must_haves:
  truths:
    - "SimpleMLP with hidden_dim=256, num_layers=5 has ~985K parameters"
    - "SimpleMLP forward(x, t) produces output shape [B, 1024]"
    - "train.py --arch mlp instantiates SimpleMLP instead of placeholder"
  artifacts:
    - path: "study/flow_matching/models/mlp.py"
      provides: "SimpleMLP velocity network class"
      exports: ["SimpleMLP", "timestep_embedding"]
    - path: "study/flow_matching/models/__init__.py"
      provides: "Model factory function"
      exports: ["create_model", "SimpleMLP"]
    - path: "study/flow_matching/train.py"
      provides: "CLI with real model instantiation"
      contains: "create_model"
  key_links:
    - from: "study/flow_matching/train.py"
      to: "study/flow_matching/models/__init__.py"
      via: "import create_model"
      pattern: "from study.flow_matching.models import create_model"
---

<objective>
Implement Simple MLP velocity network (~985K params) with sinusoidal time embedding and integrate with train.py via model factory.

Purpose: Establish first baseline architecture for flow matching experiments, replacing the placeholder SimpleVelocityNet with a properly-sized MLP for meaningful ablation studies.

Output: SimpleMLP class in models/mlp.py, model factory in models/__init__.py, updated train.py using real models.
</objective>

<execution_context>
@/home/prusek/.claude/get-shit-done/workflows/execute-plan.md
@/home/prusek/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/03-baseline-architectures/03-RESEARCH.md
@study/flow_matching/train.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create models directory and SimpleMLP class</name>
  <files>
    study/flow_matching/models/__init__.py
    study/flow_matching/models/mlp.py
  </files>
  <action>
    1. Create study/flow_matching/models/ directory

    2. Create mlp.py with:
       - timestep_embedding function (sinusoidal, from RESEARCH.md pattern)
       - SimpleMLP class with init params:
         - input_dim=1024 (SONAR embedding size)
         - hidden_dim=256 (target ~985K params)
         - num_layers=5 (moderate depth)
         - time_embed_dim=256
       - Time MLP: time_embed_dim -> hidden_dim with SiLU
       - Main network: [x, t_emb] concat -> hidden layers with SiLU -> output
       - _init_weights method: Kaiming for hidden layers, near-zero (std=0.01) for output layer
       - forward(x, t): Handle t shapes [B], [B,1], scalar; return [B, 1024]

    3. Create __init__.py with:
       - Import SimpleMLP
       - create_model(arch: str, **kwargs) factory function
       - For arch="mlp": return SimpleMLP with documented defaults
       - For unknown arch: raise ValueError with helpful message
       - __all__ = ["create_model", "SimpleMLP"]
  </action>
  <verify>
    Run: python -c "
from study.flow_matching.models import create_model, SimpleMLP
import torch
m = create_model('mlp')
p = sum(p.numel() for p in m.parameters())
print(f'SimpleMLP params: {p:,}')
assert 800_000 < p < 1_200_000, f'Param count {p} out of range'
x = torch.randn(4, 1024)
t = torch.rand(4)
v = m(x, t)
assert v.shape == (4, 1024), f'Output shape wrong: {v.shape}'
print('SimpleMLP verification passed')
"
  </verify>
  <done>SimpleMLP class exists with ~985K params and correct forward signature</done>
</task>

<task type="auto">
  <name>Task 2: Update train.py to use model factory</name>
  <files>study/flow_matching/train.py</files>
  <action>
    1. Add import: from study.flow_matching.models import create_model

    2. Replace SimpleVelocityNet instantiation with:
       model = create_model(args.arch)

    3. Remove the SimpleVelocityNet class definition (no longer needed)

    4. Update the model creation log message to show arch name from args

    5. Keep all other functionality (GPU verification, resume, Wandb, etc.)
  </action>
  <verify>
    Run: python -c "
import sys
sys.argv = ['train.py', '--arch', 'mlp', '--flow', 'icfm', '--dataset', '5k', '--group', 'test']
# Just verify imports and arg parsing work
from study.flow_matching.train import parse_args
args = parse_args()
assert args.arch == 'mlp'
from study.flow_matching.models import create_model
m = create_model(args.arch)
print(f'Model created: {type(m).__name__}')
print('train.py integration verified')
"
  </verify>
  <done>train.py uses create_model factory instead of hardcoded placeholder</done>
</task>

<task type="auto">
  <name>Task 3: Smoke test MLP training</name>
  <files></files>
  <action>
    Run a brief training smoke test (1-2 epochs) to verify:
    1. Model trains without NaN loss
    2. Gradient clipping works (no explosions)
    3. Loss decreases from initial value

    Command (use WANDB_MODE=offline to skip login):
    CUDA_VISIBLE_DEVICES=1 WANDB_MODE=offline uv run python -m study.flow_matching.train \
      --arch mlp --flow icfm --dataset 1k --group smoke-test --epochs 2

    Capture and log the output. Verify:
    - No NaN in loss values
    - Loss at epoch 2 <= loss at epoch 1
    - Training completes without error
  </action>
  <verify>
    Training log shows:
    - Epoch 1 loss is finite (not NaN/Inf)
    - Epoch 2 loss is finite and <= epoch 1 loss
    - "TRAINING COMPLETE" message appears
  </verify>
  <done>SimpleMLP trains without NaN loss and shows decreasing loss trend</done>
</task>

</tasks>

<verification>
1. SimpleMLP parameter count is in range [800K, 1.2M]
2. create_model('mlp') returns SimpleMLP instance
3. train.py --arch mlp creates SimpleMLP via factory
4. Smoke test shows training works without NaN
</verification>

<success_criteria>
- SimpleMLP velocity network with ~985K params exists
- Model factory create_model() supports 'mlp' architecture
- train.py uses factory instead of placeholder
- Brief training run completes without NaN loss
</success_criteria>

<output>
After completion, create `.planning/phases/03-baseline-architectures/03-01-SUMMARY.md`
</output>
