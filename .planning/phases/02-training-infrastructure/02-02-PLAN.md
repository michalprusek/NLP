---
phase: 02-training-infrastructure
plan: 02
type: execute
wave: 2
depends_on: ["02-01"]
files_modified:
  - study/flow_matching/trainer.py
  - study/flow_matching/utils.py
  - study/flow_matching/train.py
autonomous: true

must_haves:
  truths:
    - "Wandb logs training metrics with proper grouping"
    - "Best checkpoint saved when validation loss improves"
    - "Training can resume from checkpoint with --resume flag"
    - "Failed runs are tagged as 'failed' in Wandb"
  artifacts:
    - path: "study/flow_matching/trainer.py"
      provides: "FlowTrainer with Wandb and checkpoint integration"
      contains: "wandb.init"
    - path: "study/flow_matching/utils.py"
      provides: "Checkpoint save/load utilities"
      exports: ["save_checkpoint", "load_checkpoint"]
    - path: "study/checkpoints/"
      provides: "Checkpoint directory structure"
  key_links:
    - from: "study/flow_matching/trainer.py"
      to: "wandb"
      via: "wandb.log() calls with step parameter"
      pattern: "wandb\\.log\\("
    - from: "study/flow_matching/trainer.py"
      to: "study/flow_matching/utils.py"
      via: "save_checkpoint/load_checkpoint calls"
      pattern: "save_checkpoint|load_checkpoint"
---

<objective>
Add Wandb experiment tracking and checkpoint management to FlowTrainer.

Purpose: Enable reproducible experiments with proper tracking, checkpointing, and resumption for NeurIPS-quality ablation studies.
Output: FlowTrainer with Wandb integration, checkpoint save/load utilities, resume support in train.py.
</objective>

<execution_context>
@/home/prusek/.claude/get-shit-done/workflows/execute-plan.md
@/home/prusek/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/02-training-infrastructure/02-CONTEXT.md
@.planning/phases/02-training-infrastructure/02-RESEARCH.md
@.planning/phases/02-training-infrastructure/02-01-SUMMARY.md

# Files to modify
@study/flow_matching/trainer.py
@study/flow_matching/utils.py
@study/flow_matching/train.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Add checkpoint utilities</name>
  <files>
    study/flow_matching/utils.py
  </files>
  <action>
Add checkpoint save/load functions to utils.py:

**save_checkpoint(path, epoch, model, ema, optimizer, scheduler, best_loss, config, stats):**
- Create parent directory if needed (os.makedirs)
- Save dict with: epoch, model_state_dict, ema_shadow, optimizer_state_dict, scheduler_state_dict, best_loss, config (as dict), normalization_stats
- Verify file exists and size > 0 after save
- Log checkpoint path

**load_checkpoint(path, model, ema, optimizer, scheduler, device) -> tuple[int, float]:**
- Load checkpoint with weights_only=False (contains config dict)
- Restore model, EMA, optimizer, scheduler states
- Move tensors to correct device
- Return (start_epoch, best_loss)

**get_checkpoint_path(run_name) -> str:**
- Returns `study/checkpoints/{run_name}/best.pt`

Key points from CONTEXT.md:
- Save best checkpoint only (by validation loss)
- Local storage: `study/checkpoints/{run_name}/best.pt`
- Checkpoint includes: model state, EMA state, optimizer state, epoch, best loss
- Warmup skipped on resume (already warmed via scheduler state)
  </action>
  <verify>
```bash
cd /home/prusek/NLP && uv run python -c "
import torch
import torch.nn as nn
import tempfile
import os
from study.flow_matching.utils import (
    save_checkpoint, load_checkpoint, get_checkpoint_path,
    EMAModel, get_cosine_schedule_with_warmup
)

# Create dummy model and states
model = nn.Linear(10, 10)
ema = EMAModel(model, decay=0.9999)
optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)
scheduler = get_cosine_schedule_with_warmup(optimizer, 10, 100)

# Step a few times
for _ in range(20):
    scheduler.step()

# Save checkpoint
with tempfile.TemporaryDirectory() as tmpdir:
    path = os.path.join(tmpdir, 'test/best.pt')
    save_checkpoint(
        path=path,
        epoch=5,
        model=model,
        ema=ema,
        optimizer=optimizer,
        scheduler=scheduler,
        best_loss=0.123,
        config={'test': True},
        stats={'mean': torch.zeros(10), 'std': torch.ones(10)},
    )
    assert os.path.exists(path)
    print(f'Checkpoint saved: {os.path.getsize(path)} bytes')

    # Load checkpoint
    model2 = nn.Linear(10, 10)
    ema2 = EMAModel(model2, decay=0.9999)
    optimizer2 = torch.optim.AdamW(model2.parameters(), lr=1e-3)
    scheduler2 = get_cosine_schedule_with_warmup(optimizer2, 10, 100)

    start_epoch, best_loss = load_checkpoint(
        path, model2, ema2, optimizer2, scheduler2, torch.device('cpu')
    )

    assert start_epoch == 6  # epoch + 1
    assert abs(best_loss - 0.123) < 1e-6
    assert scheduler2.last_epoch == 20  # Restored from saved state
    print(f'Loaded: start_epoch={start_epoch}, best_loss={best_loss}')

# Test path generation
path = get_checkpoint_path('mlp-icfm-5k-none')
assert path == 'study/checkpoints/mlp-icfm-5k-none/best.pt'
print(f'Path: {path}')

print('Checkpoint utilities OK')
"
```
  </verify>
  <done>
save_checkpoint saves all required states with verification.
load_checkpoint restores all states correctly.
Scheduler state is preserved for proper warmup skip on resume.
  </done>
</task>

<task type="auto">
  <name>Task 2: Integrate Wandb into FlowTrainer</name>
  <files>
    study/flow_matching/trainer.py
  </files>
  <action>
Update FlowTrainer to integrate Wandb and checkpointing:

**New __init__ parameters:**
- wandb_project: str = "flow-matching-study"
- resume_path: Optional[str] = None

**_setup() updates:**
1. Initialize Wandb with:
   - project=wandb_project
   - group=config.group (e.g., 'flow-methods', 'architectures')
   - name=config.run_name
   - config=config.to_dict()
   - resume="allow" if resume_path else "never"

2. Define metrics:
   - wandb.define_metric("val/loss", summary="min")
   - wandb.define_metric("train/loss", summary="min")

3. If resume_path provided:
   - Call load_checkpoint to restore states
   - Set start_epoch, best_val_loss from checkpoint
   - Log "Resumed from epoch {N}"

**train_epoch() updates:**
- Track global_step
- Log every 10 steps: train/loss, train/grad_norm, train/lr, epoch
- Use wandb.log(..., step=global_step) for proper alignment

**validate() updates:**
- No changes (no logging here)

**train() updates:**
1. After validation each epoch:
   - If improved: save_checkpoint to get_checkpoint_path(config.run_name)
   - Log val/loss, val/best_loss with step=global_step

2. After training or early stop:
   - wandb.summary["final_epoch"] = epoch
   - wandb.summary["best_val_loss"] = best_val_loss
   - wandb.summary["checkpoint_path"] = checkpoint_path

3. Wrap in try/except:
   - On exception: tag run as "failed", set notes, call wandb.finish(exit_code=1)
   - On success: call wandb.finish()

**New method finish():**
- Call wandb.finish() for clean shutdown
  </action>
  <verify>
```bash
cd /home/prusek/NLP && CUDA_VISIBLE_DEVICES=1 WANDB_MODE=offline uv run python -c "
import torch
import torch.nn as nn
import wandb
from study.flow_matching.config import TrainingConfig
from study.flow_matching.trainer import FlowTrainer
from study.data.dataset import FlowDataset

# Simple velocity network
class SimpleVelocityNet(nn.Module):
    def __init__(self, dim=1024, hidden=256):
        super().__init__()
        self.net = nn.Sequential(
            nn.Linear(dim + 1, hidden),
            nn.SiLU(),
            nn.Linear(hidden, dim),
        )
    def forward(self, x, t):
        t = t.unsqueeze(-1) if t.dim() == 1 else t
        return self.net(torch.cat([x, t], dim=-1))

# Load small dataset
train_ds = FlowDataset('study/datasets/splits/1k/train.pt')
val_ds = FlowDataset('study/datasets/splits/1k/val.pt')

# Create config
config = TrainingConfig(
    arch='test', flow='icfm', dataset='1k', aug='none', group='test',
    epochs=2, batch_size=64, lr=1e-3, warmup_steps=10, patience=5
)

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
model = SimpleVelocityNet()

# Train with Wandb (offline mode)
trainer = FlowTrainer(
    model, config, train_ds, val_ds, device,
    wandb_project='flow-matching-study'
)

result = trainer.train()
print(f'Trained {result[\"epochs_run\"]} epochs')
print(f'Best val loss: {result[\"best_val_loss\"]:.4f}')

# Check checkpoint was saved
import os
ckpt_path = 'study/checkpoints/test-icfm-1k-none/best.pt'
if os.path.exists(ckpt_path):
    print(f'Checkpoint saved: {os.path.getsize(ckpt_path)} bytes')
else:
    print('WARNING: Checkpoint not saved (may be OK if no improvement)')

print('Wandb integration OK')
"
```
  </verify>
  <done>
FlowTrainer initializes Wandb with proper grouping.
Training logs metrics every 10 steps with correct step alignment.
Best checkpoint saved when validation improves.
Failed runs tagged in Wandb.
  </done>
</task>

<task type="auto">
  <name>Task 3: Add resume support to train.py</name>
  <files>
    study/flow_matching/train.py
  </files>
  <action>
Update train.py to support resumption:

**New CLI arguments:**
- --resume PATH: Path to checkpoint to resume from
- --wandb-project: Wandb project name (default: flow-matching-study)

**Main function updates:**
1. If --resume provided:
   - Verify checkpoint file exists
   - Pass resume_path to FlowTrainer

2. Print training summary at end:
   - Total epochs, best val loss, checkpoint path
   - Training time

3. Add proper error handling:
   - Catch exceptions, log error, exit 1

**Test resumption:**
- Train for 2 epochs, save checkpoint
- Resume from checkpoint, train 2 more epochs
- Verify epoch counter continues correctly
  </action>
  <verify>
```bash
cd /home/prusek/NLP && CUDA_VISIBLE_DEVICES=1 WANDB_MODE=offline uv run python -c "
import subprocess
import os
import sys

# Clean up any existing checkpoint
ckpt_dir = 'study/checkpoints/resume-test-icfm-1k-none'
os.makedirs(ckpt_dir, exist_ok=True)

# Run 1: Train for 2 epochs
print('=== Run 1: Initial training ===')
result = subprocess.run([
    sys.executable, '-m', 'study.flow_matching.train',
    '--arch', 'resume-test', '--flow', 'icfm', '--dataset', '1k',
    '--group', 'test', '--epochs', '2', '--batch-size', '64', '--lr', '1e-3',
    '--wandb-project', 'flow-matching-study'
], capture_output=True, text=True, env={**os.environ, 'WANDB_MODE': 'offline'})
print(result.stdout[-500:] if len(result.stdout) > 500 else result.stdout)
if result.returncode != 0:
    print('STDERR:', result.stderr[-500:])

# Check checkpoint exists
ckpt_path = f'{ckpt_dir}/best.pt'
assert os.path.exists(ckpt_path), f'Checkpoint not found: {ckpt_path}'
print(f'Checkpoint saved: {ckpt_path}')

# Run 2: Resume and train 2 more epochs
print('\\n=== Run 2: Resumed training ===')
result = subprocess.run([
    sys.executable, '-m', 'study.flow_matching.train',
    '--arch', 'resume-test', '--flow', 'icfm', '--dataset', '1k',
    '--group', 'test', '--epochs', '4', '--batch-size', '64', '--lr', '1e-3',
    '--resume', ckpt_path,
    '--wandb-project', 'flow-matching-study'
], capture_output=True, text=True, env={**os.environ, 'WANDB_MODE': 'offline'})
print(result.stdout[-500:] if len(result.stdout) > 500 else result.stdout)
if result.returncode != 0:
    print('STDERR:', result.stderr[-500:])

print('Resume support OK')
"

# Cleanup test checkpoint
rm -rf /home/prusek/NLP/study/checkpoints/resume-test-icfm-1k-none 2>/dev/null || true
```
  </verify>
  <done>
train.py supports --resume flag for checkpoint resumption.
Epoch counter continues from checkpoint.
Scheduler state preserved (no warmup restart).
  </done>
</task>

</tasks>

<verification>
Full end-to-end verification:

```bash
cd /home/prusek/NLP && CUDA_VISIBLE_DEVICES=1 WANDB_MODE=offline uv run python -m study.flow_matching.train \
  --arch verify --flow icfm --dataset 1k --group test \
  --epochs 3 --batch-size 64 --lr 1e-3 \
  --wandb-project flow-matching-study 2>&1 | tail -30

# Verify checkpoint was created
ls -la /home/prusek/NLP/study/checkpoints/verify-icfm-1k-none/ 2>/dev/null || echo "No checkpoint (expected if no improvement)"

# Cleanup
rm -rf /home/prusek/NLP/study/checkpoints/verify-icfm-1k-none 2>/dev/null || true
rm -rf /home/prusek/NLP/wandb 2>/dev/null || true
```
</verification>

<success_criteria>
1. Wandb initializes with project="flow-matching-study" and proper group/name
2. Training metrics logged every 10 steps with step parameter
3. Validation metrics logged every epoch
4. Best checkpoint saved to study/checkpoints/{run_name}/best.pt
5. Resume from checkpoint works with --resume flag
6. Failed runs tagged as "failed" in Wandb
7. Wandb summary contains final_epoch, best_val_loss, checkpoint_path
</success_criteria>

<output>
After completion, create `.planning/phases/02-training-infrastructure/02-02-SUMMARY.md`
</output>
