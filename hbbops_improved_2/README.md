# HbBoPs: Hyperband-based Bayesian Optimization for Black-box Prompt Selection

Komplexn√≠ implementace paperu **"Hyperband-based Bayesian Optimization for Black-box Prompt Selection"** (Schneider et al., 2025) pro GSM8K dataset.

## üìã P≈ôehled

HbBoPs kombinuje t≈ôi kl√≠ƒçov√© komponenty pro efektivn√≠ v√Ωbƒõr prompt≈Ø:

1. **Structural-aware Deep Kernel GP** - pro sample-efficiency (m√©nƒõ evaluac√≠ prompt≈Ø)
2. **Hyperband multi-fidelity scheduler** - pro query-efficiency (m√©nƒõ LLM calls)
3. **Bayesian Optimization** - pro inteligentn√≠ v√Ωbƒõr kandid√°tn√≠ch prompt≈Ø

## üèóÔ∏è Architektura

### 1. Structural-aware Deep Kernel GP

```
Prompt = Instruction + Few-shot Exemplar
         ‚Üì              ‚Üì
    BERT [CLS]     BERT [CLS]
    (768 dim)      (768 dim)
         ‚Üì              ‚Üì
    Lin(768,64)    Lin(768,64)
       ‚Üí ReLU         ‚Üí ReLU
    Lin(64,32)     Lin(64,32)
       ‚Üí ReLU         ‚Üí ReLU
         ‚Üì              ‚Üì
    Concat (32 + 32 = 64 dim)
              ‚Üì
         Lin(64,32)
            ‚Üí ReLU
         Lin(32,10)
              ‚Üì
    Latent Features (10 dim)
              ‚Üì
    GP with ARD Mat√©rn 5/2 kernel
```

**Kl√≠ƒçov√© vlastnosti:**
- Separ√°tn√≠ embeddingy pro instrukce a exempl√°≈ôe
- Uƒç√≠ se low-dimensional (10-dim) latent representation zarovnanou s performance prompt≈Ø
- Tr√©nuje se online bƒõhem optimalizace pomoc√≠ AdamW (lr=0.01, max_epochs=3000, patience=10)

### 2. Hyperband Scheduler

**Multi-fidelity over validation instances:**

- **Fidelity** = poƒçet validaƒçn√≠ch instanc√≠ pou≈æit√Ωch k evaluaci promptu
- **bmin** = 10 (minim√°ln√≠ poƒçet instanc√≠)
- **Œ∑** = 2.0 (halving parameter)

**P≈ô√≠klad Hyperband schedule (nvalid=1319):**

| Bracket (s) | Stage (i) | #Instances (b) | #Prompts (n) |
|-------------|-----------|----------------|--------------|
| 3           | 0         | 10             | 8            |
| 3           | 1         | 20             | 4            |
| 3           | 2         | 40             | 2            |
| 3           | 3         | 80             | 1            |
| 2           | 0         | 20             | 6            |
| 2           | 1         | 40             | 3            |
| ...         | ...       | ...            | ...          |

**D≈Øle≈æit√© design decisions:**
- ‚úÖ Pou≈æit√≠ stejn√Ωch validation instanc√≠ pro v≈°echny prompty v stage (paired comparison)
- ‚úÖ Superset struktura: vy≈°≈°√≠ stages roz≈°i≈ôuj√≠ ni≈æ≈°√≠ (ne resample)
- ‚úÖ Caching evaluac√≠ pro zrychlen√≠
- ‚úÖ Incumbent = prompt s nejni≈æ≈°√≠ val. error mezi tƒõmi evaluovan√Ωmi na full validation set

### 3. Bayesian Optimization Proposal

- **Acquisition function:** Expected Improvement (EI)
- **Random interleaving:** œÅ = 0.1 (hedge against ≈°patn√© GP predikce)
- GP tr√©novan√Ω na nejvy≈°≈°√≠ fidelity level s ‚â•4 observations

## üìÅ Struktura Soubor≈Ø

```
hbbops/
‚îú‚îÄ‚îÄ instructions.txt                 # 5 instrukc√≠ pro GSM8K (APE forward mode)
‚îú‚îÄ‚îÄ examples.txt                     # 50 exempl√°≈ô≈Ø (25 set≈Ø √ó 2 permutace)
‚îú‚îÄ‚îÄ data/
‚îÇ   ‚îú‚îÄ‚îÄ validation.json              # 1319 examples (randomly sampled z train)
‚îÇ   ‚îî‚îÄ‚îÄ test.json                    # 1319 examples (original test set)
‚îú‚îÄ‚îÄ hbbops.py                        # Hlavn√≠ implementace HbBoPs
‚îú‚îÄ‚îÄ run_hbbops.py                    # Main script pro spu≈°tƒõn√≠
‚îî‚îÄ‚îÄ README.md                        # Dokumentace
```

## üöÄ Instalace

```bash
# Z√°vislosti jsou v pyproject.toml
uv sync
```

**Pot≈ôebn√© dependencies:**
- torch
- gpytorch
- transformers
- scipy
- numpy
- datasets

## üíª Pou≈æit√≠

### Spu≈°tƒõn√≠ HbBoPs

**Z√°kladn√≠ pou≈æit√≠:**
```bash
cd hbbops
uv run python run_hbbops.py --model Qwen/Qwen2.5-7B-Instruct --backend vllm
```

**S r≈Øzn√Ωmi parametry:**
```bash
cd hbbops

# Men≈°√≠ model na CPU
uv run python run_hbbops.py \
    --model Qwen/Qwen2.5-3B-Instruct \
    --backend transformers \
    --device cpu \
    --bmin 5 \
    --eta 2.0

# Claude API
uv run python run_hbbops.py \
    --model claude-3-haiku-20240307 \
    --backend claude \
    --encoder bert-base-uncased

# Debug mode
uv run python run_hbbops.py \
    --model Qwen/Qwen2.5-7B-Instruct \
    --debug
```

**Parametry:**
- `--model`: Model name nebo path (default: Qwen/Qwen2.5-7B-Instruct)
- `--backend`: Backend pro LLM - `vllm`, `transformers`, `claude` (default: vllm)
- `--bmin`: Minim√°ln√≠ poƒçet validation instanc√≠ (default: 10)
- `--eta`: Halving parameter pro Hyperband (default: 2.0)
- `--encoder`: Encoder model pro embeddingy (default: bert-base-uncased)
- `--device`: Device - `auto`, `cuda`, `cpu`, `mps` (default: auto)
- `--debug`: Enable debug mode (zobraz√≠ LLM odpovƒõdi a evaluace)
- `--output-dir`: Output directory pro v√Ωsledky (default: results)

### 3. V√Ωstupy

**JSON soubor (`results/hbbops_TIMESTAMP.json`):**
```json
{
  "method": "HbBoPs",
  "model": "Qwen/Qwen2.5-7B-Instruct",
  "config": {
    "bmin": 10,
    "eta": 2.0,
    "num_prompts": 250
  },
  "best_prompt": {
    "instruction_id": 2,
    "exemplar_id": 15,
    "instruction": "...",
    "exemplar": "..."
  },
  "validation_error": 0.1234,
  "test_error": 0.1456
}
```

**TXT soubor (`results/hbbops_TIMESTAMP.txt`):**
```
HbBoPs Results
================================================================================

Model: Qwen/Qwen2.5-7B-Instruct
Backend: vllm

Validation error: 0.1234 (12.34%)
Test error: 0.1456 (14.56%)

Best Prompt:
--------------------------------------------------------------------------------
[cel√Ω prompt text]
```

## üî¨ Implementaƒçn√≠ Detaily

### Search Space

**5 instructions √ó 50 exemplars = 250 candidate prompts**

**Instructions (5):**
- Generovan√© pomoc√≠ APE forward mode s Claude 3 Sonnet
- Z 10 input-output p≈ô√≠klad≈Ø z GSM8K train setu

**Exemplars (50):**
- 25 set≈Ø po 5 input-output examples
- Ka≈æd√Ω set permutov√°n 2√ó ‚Üí 50 exempl√°≈ô≈Ø celkem
- Testuje vliv po≈ôad√≠ examples na performance

### Encoder

- **BERT base-uncased** pro embeddingy (768 dim)
- [CLS] token embedding
- Separ√°tnƒõ pro instrukce a exempl√°≈ôe

### GP Training

- **Optimizer:** AdamW (lr=0.01)
- **Max epochs:** 3000
- **Early stopping:** patience=10
- **Loss:** Negative log marginal likelihood
- **Data normalization:**
  - Input: Z-score normalization (zero mean, unit variance)
  - Output: Z-score normalization

### Evaluace Prompt≈Ø

**Answer Extraction (priority order):**
1. `final_answer: NUMBER` pattern
2. `#### NUMBER` pattern (GSM8K format)
3. `\boxed{NUMBER}` pattern (LaTeX)
4. Last number in text (fallback)

**Scoring:**
- Exact match s numerical tolerance (1e-4)
- Validation error = fraction of incorrect answers

## üìä Expected Performance

Podle paperu (pr≈Ømƒõr p≈ôes 10 benchmarks a 3 LLMs):
- **HbBoPs:** 0.150 normalized test error @ full budget
- **TRIPLE-SH (nejbli≈æ≈°√≠ konkurent):** 0.159
- **Improvement:** ~6% lep≈°√≠ ne≈æ TRIPLE-SH

**Anytime performance:**
- @ 0.25 budget: 24% lep≈°√≠ ne≈æ TRIPLE-SH
- @ 0.50 budget: 21% lep≈°√≠ ne≈æ TRIPLE-SH

## üéØ Kl√≠ƒçov√© V√Ωhody HbBoPs

1. **Sample-efficient:** Structural-aware DK-GP efektivnƒõ exploruje search space
2. **Query-efficient:** Hyperband rychle filtruje ≈°patn√© prompty s mal√Ωm poƒçtem instanc√≠
3. **Both:** Kombinace BO + Hyperband je sample- i query-efficient
4. **Structural awareness:** Separ√°tn√≠ embeddingy zachycuj√≠ odli≈°nou strukturu instrukc√≠ vs. exempl√°≈ô≈Ø

## üîß Troubleshooting

**Out of Memory:**
```bash
# Pou≈æij men≈°√≠ model
--model Qwen/Qwen2.5-3B-Instruct

# Pou≈æij CPU
--device cpu --backend transformers

# Sn√≠≈æit bmin (m√©nƒõ validation instanc√≠)
--bmin 5
```

**Slow training:**
```bash
# Pou≈æij vLLM m√≠sto transformers
--backend vllm

# Pou≈æij GPU
--device cuda
```

**GP training issues:**
```bash
# Debug mode uk√°≈æe GP training progress
--debug
```

## üìö Reference

```
@inproceedings{schneider2025hbbops,
  title={Hyperband-based Bayesian Optimization for Black-box Prompt Selection},
  author={Schneider, Lennart and Wistuba, Martin and Klein, Aaron and
          Golebiowski, Jacek and Zappella, Giovanni and Merra, Felice Antonio},
  booktitle={Proceedings of the 42nd International Conference on Machine Learning},
  year={2025}
}
```

## üìù Notes

- **Reproducibility:** Random seed je fixov√°n v setup scriptu (seed=42)
- **Validation split:** 1319 examples n√°hodnƒõ samplov√°ny z train setu (stejn√° velikost jako test)
- **Caching:** LLM outputs jsou cachov√°ny - restart nep≈ôehodnot√≠ ji≈æ evaluovan√© prompty
- **Multi-GPU:** Pro vLLM backend lze pou≈æ√≠t `--tensor-parallel-size` v llm_client.py

## üêõ Known Limitations

1. **Small encoder models:** BERT base m√° omezen√≠ na 512 token≈Ø - dlouh√© exempl√°≈ôe jsou truncated
2. **GP scaling:** Pro >1000 observations m≈Ø≈æe b√Ωt GP training pomal√Ω
3. **Memory:** Ukl√°d√°n√≠ v≈°ech embeddings v pamƒõti m≈Ø≈æe b√Ωt probl√©m pro velk√© search spaces

## üöß Future Work

- [ ] Podpora pro v√≠ce encoder models (MPNet, DistilRoBERTa)
- [ ] Parallelizace Hyperband brackets
- [ ] Support pro dal≈°√≠ datasets (ARC, BBII tasks)
- [ ] Vizualizace GP latent space (t-SNE)
- [ ] Comparison s baseline methods (EASE, MIPROv2, TRIPLE)
