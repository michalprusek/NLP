You are a CRITIC for Prompt Optimization.
Analyze the current prompt's performance and provide SHORT, ACTIONABLE feedback for improvement.

CONTEXT
- TASK: {task_description}
- CURRENT_PROMPT:
<<<
{prompt}
>>>
- PERFORMANCE:
  * Accuracy: {accuracy:.2%} (correct={correct}, total={total}, minibatch={num_examples})
  * Top failure examples:
{results}

IMPORTANT - How classifications are evaluated:

The evaluation extracts labels using these patterns (prioritized):
1. "final_labels: 0, 4, 7" or "Labels: [0, 4, 7]"
2. "Categories: Arbitration, Limitation of liability"
3. Label names mentioned anywhere in text
4. Any numbers 0-8 in the output (fallback)

CRITICAL: This is MULTI-LABEL classification - each clause can have MULTIPLE labels (0-9 labels).
Accuracy requires EXACT MATCH - all predicted labels must match all true labels (no more, no less).

DATASET CHARACTERISTICS:
- **90% of clauses have NO labels** (all fields false) - these are metadata, headers, general text
- **9% have exactly 1 label** - single unfair clause type
- **<1% have 2+ labels** - multiple unfair clause types (e.g., "termination + change")
- Models must distinguish unfair clauses from neutral/informational text
- Empty label set (LABELS: NONE or no output) is a VALID and COMMON answer

LABEL CATEGORIES (9 total):
0: Limitation of liability (ltd) - Clauses limiting provider liability for damages
1: Unilateral termination (ter) - Provider can terminate service/account without cause
2: Unilateral change (ch) - Provider can change terms/service unilaterally
3: Arbitration (a) - Disputes must go to arbitration (not court)
4: Content removal (cr) - Provider can remove/delete user content
5: Choice of law (law) - Specifies which jurisdiction's laws apply to agreement
6: Other (pinc) - Potentially unfair clauses not fitting other categories
7: Contract by using (use) - Using service creates binding contract (no explicit acceptance)
8: Jurisdiction (j) - Specifies where legal disputes must be filed

KEY INSIGHTS:
- **MULTI-LABEL**: A single clause can have 0, 1, or MULTIPLE labels simultaneously
- **EXACT MATCH**: Missing one label OR adding one extra label = 0% accuracy for that example
- **MOST CLAUSES ARE NEUTRAL**: 90% have no unfair terms - don't over-classify!
- Models must distinguish unfair clauses from informational/neutral text
- Models must analyze legal language carefully and identify ALL applicable categories
- Chain-of-thought reasoning before classification helps accuracy
- Output must be explicit and complete: "LABELS: 0, 3, 5" or "LABELS: NONE" (all applicable labels)
- Common errors: missing secondary labels, misinterpreting legal terminology, confusing similar categories, over-classifying neutral text

COMMON FAILURE PATTERNS:
- Vague or missing output format guidance → model doesn't provide extractable labels
- No reasoning step → model jumps to wrong conclusion or misses labels
- **Over-classification** → labeling neutral/informational text as unfair (90% should be NONE!)
- **Missing secondary labels** → model identifies primary category but misses others
- **Not outputting NONE** → model forces a label even when clause is neutral
- Confusing category 1 (termination) vs category 7 (contract by using)
- Confusing category 3 (arbitration) vs category 8 (jurisdiction)
- Confusing category 2 (unilateral change) vs category 1 (unilateral termination)
- Category 6 (Other) used as catch-all when uncertain
- **Predicting only one label when multiple apply** → treating as single-label task

YOUR TASK:
Provide a concise critique identifying 2-4 key issues with the current prompt that led to failures.
Focus on: output format guidance, reasoning structure, category disambiguation.

FORMAT YOUR RESPONSE AS:

ISSUE 1: [Brief description of the problem]
- Root cause: [Why this causes failures]
- Suggested improvements:
  * [Specific action 1]
  * [Specific action 2]

ISSUE 2: [Brief description of the problem]
- Root cause: [Why this causes failures]
- Suggested improvements:
  * [Specific action 1]

[Continue for 2-4 issues total]

GLOBAL NOTES:
- [Important constraint or guideline]

CONSTRAINTS:
- Keep suggestions compact (≈300-450 tokens total)
- Do NOT propose changes that break the output format
- Focus on systematic issues affecting category confusion
- Ensure prompt guides clear "LABEL: <number>" output
