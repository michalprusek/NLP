"""Improved GP Models for High-Dimensional Optimization.

Implements lessons learned from:
1. Hvarfner et al. 2024 "Vanilla BO Performs Great in High Dimensions"
2. GP Benchmark analysis showing need for large lengthscales

Models:
- VanillaGP: Standard GP with dimension-scaled LogNormal prior
- ImprovedSAAS: SAAS with minimum lengthscale constraint
- DKL10: Deep Kernel Learning with 10D feature extraction
"""

import logging
import math
import time
from dataclasses import dataclass
from typing import List, Optional, Tuple

import torch
import torch.nn as nn
import gpytorch
from gpytorch.kernels import MaternKernel, RBFKernel, ScaleKernel
from gpytorch.likelihoods import GaussianLikelihood
from gpytorch.means import ConstantMean
from gpytorch.models import ExactGP
from gpytorch.mlls import ExactMarginalLogLikelihood
from gpytorch.priors import LogNormalPrior
from gpytorch.constraints import GreaterThan

logger = logging.getLogger(__name__)

DTYPE = torch.float64
SQRT2 = math.sqrt(2)
SQRT3 = math.sqrt(3)


# =============================================================================
# VanillaGP with Dimension-Scaled Prior (Hvarfner 2024)
# =============================================================================

@dataclass
class VanillaGPConfig:
    """Configuration for VanillaGP."""
    lr: float = 0.1
    epochs: int = 500
    patience: int = 50
    jitter: float = 1e-4
    use_ard: bool = True
    kernel_type: str = "rbf"  # "rbf" or "matern"
    min_lengthscale: float = 0.1  # Minimum lengthscale constraint


class VanillaGPModel(ExactGP):
    """GPyTorch model with dimension-scaled priors (Hvarfner 2024)."""

    def __init__(
        self,
        train_x: torch.Tensor,
        train_y: torch.Tensor,
        likelihood: gpytorch.likelihoods.Likelihood,
        dim: int,
        use_ard: bool = True,
        kernel_type: str = "rbf",
        min_lengthscale: float = 0.1,
    ):
        super().__init__(train_x, train_y, likelihood)

        self.mean_module = ConstantMean()

        # Dimension-scaled LogNormal prior (Hvarfner 2024)
        # loc = sqrt(2) + log(dim) / 2
        # scale = sqrt(3)
        ls_loc = SQRT2 + math.log(dim) * 0.5
        ls_scale = SQRT3

        ard_dims = dim if use_ard else None

        # Select kernel (without prior - we'll set it after moving to device)
        if kernel_type == "rbf":
            base_kernel = RBFKernel(
                ard_num_dims=ard_dims,
                lengthscale_constraint=GreaterThan(min_lengthscale),
            )
        else:
            base_kernel = MaternKernel(
                nu=2.5,
                ard_num_dims=ard_dims,
                lengthscale_constraint=GreaterThan(min_lengthscale),
            )

        # Store prior parameters for later registration
        self._ls_loc = ls_loc
        self._ls_scale = ls_scale
        self._use_ard = use_ard
        self._dim = dim

        self.covar_module = ScaleKernel(base_kernel)

        # Initialize lengthscales to prior median
        # Median of LogNormal(loc, scale) = exp(loc)
        init_ls = math.exp(ls_loc)
        self.covar_module.base_kernel.lengthscale = init_ls

        logger.info(
            f"VanillaGP: dim={dim}, kernel={kernel_type}, "
            f"ls_prior=LogNormal({ls_loc:.2f}, {ls_scale:.2f}), "
            f"init_ls={init_ls:.2f}"
        )

    def register_prior_on_device(self, device):
        """Register lengthscale prior after moving model to device."""
        ls_loc = self._ls_loc
        ls_scale = self._ls_scale
        dim = self._dim
        use_ard = self._use_ard

        # Create prior tensors on the correct device
        if use_ard:
            loc_tensor = torch.full((dim,), ls_loc, device=device, dtype=DTYPE)
            scale_tensor = torch.full((dim,), ls_scale, device=device, dtype=DTYPE)
        else:
            loc_tensor = torch.tensor(ls_loc, device=device, dtype=DTYPE)
            scale_tensor = torch.tensor(ls_scale, device=device, dtype=DTYPE)

        # Register the prior
        self.covar_module.base_kernel.register_prior(
            "lengthscale_prior",
            LogNormalPrior(loc=loc_tensor, scale=scale_tensor),
            lambda m: m.lengthscale,
            lambda m, v: m._set_lengthscale(v),
        )

    def forward(self, x):
        mean = self.mean_module(x)
        covar = self.covar_module(x)
        return gpytorch.distributions.MultivariateNormal(mean, covar)


class VanillaGP:
    """Vanilla GP with dimension-scaled LogNormal prior.

    Based on Hvarfner et al. 2024: "Vanilla Bayesian Optimization
    Performs Great in High Dimensions"

    Key insight: The lengthscale prior should scale with sqrt(dim) to
    prevent overfitting in high dimensions.
    """

    def __init__(
        self,
        config: Optional[VanillaGPConfig] = None,
        device: str = "cuda",
    ):
        self.config = config or VanillaGPConfig()
        self.device = torch.device(device)

        self.model: Optional[VanillaGPModel] = None
        self.likelihood: Optional[GaussianLikelihood] = None
        self.X_train: Optional[torch.Tensor] = None
        self.y_train: Optional[torch.Tensor] = None

        # Normalization stats
        self._X_mean: Optional[torch.Tensor] = None
        self._X_std: Optional[torch.Tensor] = None
        self._y_mean: float = 0.0
        self._y_std: float = 1.0

        # Training stats
        self.training_time: float = 0.0
        self.final_loss: float = float('inf')
        self.lengthscales: Optional[torch.Tensor] = None

    def _normalize_X(self, X: torch.Tensor) -> torch.Tensor:
        """Z-score normalize inputs."""
        if self._X_mean is None:
            self._X_mean = X.mean(dim=0)
            self._X_std = X.std(dim=0).clamp(min=1e-6)

        X_mean = self._X_mean.to(X.device)
        X_std = self._X_std.to(X.device)
        return (X - X_mean) / X_std

    def _standardize_y(self, y: torch.Tensor) -> torch.Tensor:
        """Standardize targets."""
        if self._y_mean == 0.0 and self._y_std == 1.0:
            self._y_mean = y.mean().item()
            self._y_std = y.std().item()
            if self._y_std < 1e-6:
                self._y_std = 1.0

        return (y - self._y_mean) / self._y_std

    def _destandardize_y(self, mean: torch.Tensor, std: torch.Tensor):
        """Convert back to original scale."""
        return mean * self._y_std + self._y_mean, std * self._y_std

    def fit(self, X: torch.Tensor, y: torch.Tensor) -> bool:
        """Fit GP via MLE with dimension-scaled prior."""
        start_time = time.time()

        X = X.to(device=self.device, dtype=DTYPE)
        y = y.to(device=self.device, dtype=DTYPE)

        self.X_train = X
        self.y_train = y

        # Normalize
        X_norm = self._normalize_X(X)
        y_norm = self._standardize_y(y)

        dim = X.shape[1]

        # Create likelihood with LogNormal noise prior (lower noise preferred)
        self.likelihood = GaussianLikelihood(
            noise_prior=LogNormalPrior(loc=torch.tensor(-4.0), scale=torch.tensor(1.0)),
            noise_constraint=GreaterThan(1e-6),
        ).to(self.device, dtype=DTYPE)

        # Create model
        self.model = VanillaGPModel(
            X_norm, y_norm, self.likelihood,
            dim=dim,
            use_ard=self.config.use_ard,
            kernel_type=self.config.kernel_type,
            min_lengthscale=self.config.min_lengthscale,
        ).to(self.device, dtype=DTYPE)

        # Register prior on correct device (after .to() call)
        self.model.register_prior_on_device(self.device)

        # Training
        self.model.train()
        self.likelihood.train()

        optimizer = torch.optim.Adam(self.model.parameters(), lr=self.config.lr)
        mll = ExactMarginalLogLikelihood(self.likelihood, self.model)

        best_loss = float('inf')
        patience_counter = 0

        with gpytorch.settings.cholesky_jitter(self.config.jitter):
            for epoch in range(self.config.epochs):
                try:
                    optimizer.zero_grad()
                    output = self.model(X_norm)
                    loss = -mll(output, y_norm)

                    if torch.isnan(loss):
                        logger.error(f"NaN loss at epoch {epoch}")
                        return False

                    loss.backward()
                    optimizer.step()

                    if loss.item() < best_loss:
                        best_loss = loss.item()
                        patience_counter = 0
                    else:
                        patience_counter += 1

                    if patience_counter >= self.config.patience:
                        logger.info(f"Early stopping at epoch {epoch + 1}")
                        break

                except RuntimeError as e:
                    if "cholesky" in str(e).lower():
                        logger.error(f"Cholesky failed at epoch {epoch}")
                        return False
                    raise

        self.training_time = time.time() - start_time
        self.final_loss = best_loss

        # Extract lengthscales
        self.lengthscales = self.model.covar_module.base_kernel.lengthscale.detach()
        ls = self.lengthscales.squeeze()
        logger.info(
            f"VanillaGP fitted in {self.training_time:.1f}s, loss={best_loss:.4f}, "
            f"ls_range=[{ls.min():.2f}, {ls.max():.2f}], ls_median={ls.median():.2f}"
        )

        self.model.eval()
        self.likelihood.eval()

        return True

    def predict(self, X: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:
        """Make predictions."""
        if self.model is None:
            raise RuntimeError("GP not fitted")

        X = X.to(device=self.device, dtype=DTYPE)
        X_norm = self._normalize_X(X)

        self.model.eval()
        self.likelihood.eval()

        with torch.no_grad(), gpytorch.settings.fast_pred_var():
            pred = self.model(X_norm)
            mean_norm = pred.mean
            std_norm = pred.variance.sqrt()

        mean, std = self._destandardize_y(mean_norm, std_norm)
        return mean.clamp(0, 1), std.clamp(min=1e-6)


# =============================================================================
# DKL-10D: Deep Kernel Learning with 10D Feature Extraction
# =============================================================================

@dataclass
class DKL10Config:
    """Configuration for DKL-10D."""
    hidden_dims: List[int] = None  # Default: [256, 64]
    feature_dim: int = 10
    lr: float = 0.001
    epochs: int = 1000
    patience: int = 100
    jitter: float = 1e-3
    dropout: float = 0.1

    def __post_init__(self):
        if self.hidden_dims is None:
            self.hidden_dims = [256, 64]


class FeatureExtractor10D(nn.Module):
    """Neural network mapping 1024D -> 10D."""

    def __init__(
        self,
        input_dim: int = 1024,
        hidden_dims: List[int] = None,
        output_dim: int = 10,
        dropout: float = 0.1,
    ):
        super().__init__()

        if hidden_dims is None:
            hidden_dims = [256, 64]

        layers = []
        prev_dim = input_dim

        for hidden_dim in hidden_dims:
            layers.extend([
                nn.Linear(prev_dim, hidden_dim),
                nn.LayerNorm(hidden_dim),
                nn.ReLU(),
                nn.Dropout(dropout),
            ])
            prev_dim = hidden_dim

        layers.append(nn.Linear(prev_dim, output_dim))

        self.net = nn.Sequential(*layers)
        self.output_dim = output_dim

    def forward(self, x):
        return self.net(x)


class DKL10Model(ExactGP):
    """GPyTorch DKL model with 10D features."""

    def __init__(
        self,
        train_x: torch.Tensor,
        train_y: torch.Tensor,
        likelihood: gpytorch.likelihoods.Likelihood,
        feature_extractor: FeatureExtractor10D,
    ):
        super().__init__(train_x, train_y, likelihood)

        self.feature_extractor = feature_extractor
        feature_dim = feature_extractor.output_dim

        self.mean_module = ConstantMean()

        # For 10D, use moderate lengthscale prior
        # LogNormal(sqrt(2) + log(10)/2, sqrt(3)) -> median â‰ˆ 5.4
        ls_loc = SQRT2 + math.log(feature_dim) * 0.5
        ls_scale = SQRT3

        # Create kernel without prior (will register after moving to device)
        self.covar_module = ScaleKernel(
            RBFKernel(
                ard_num_dims=feature_dim,
                lengthscale_constraint=GreaterThan(0.1),
            )
        )

        # Store for later
        self._ls_loc = ls_loc
        self._ls_scale = ls_scale
        self._feature_dim = feature_dim

        # Initialize lengthscales
        self.covar_module.base_kernel.lengthscale = math.exp(ls_loc)

    def register_prior_on_device(self, device):
        """Register lengthscale prior after moving to device."""
        loc = torch.full((self._feature_dim,), self._ls_loc, device=device, dtype=torch.float32)
        scale = torch.full((self._feature_dim,), self._ls_scale, device=device, dtype=torch.float32)

        self.covar_module.base_kernel.register_prior(
            "lengthscale_prior",
            LogNormalPrior(loc=loc, scale=scale),
            lambda m: m.lengthscale,
            lambda m, v: m._set_lengthscale(v),
        )

    def forward(self, x):
        features = self.feature_extractor(x)
        mean = self.mean_module(features)
        covar = self.covar_module(features)
        return gpytorch.distributions.MultivariateNormal(mean, covar)


class DKL10:
    """Deep Kernel Learning with 10D feature space.

    Maps 1024D SONAR embeddings to 10D learned features, then applies
    GP with dimension-scaled prior in 10D space.

    Benefits:
    - 10D is tractable for GP modeling
    - Learns task-specific features
    - Dimension-scaled prior prevents overconfidence
    """

    def __init__(
        self,
        config: Optional[DKL10Config] = None,
        device: str = "cuda",
    ):
        self.config = config or DKL10Config()
        self.device = torch.device(device)

        self.model: Optional[DKL10Model] = None
        self.likelihood: Optional[GaussianLikelihood] = None
        self.feature_extractor: Optional[FeatureExtractor10D] = None

        self.X_train: Optional[torch.Tensor] = None
        self.y_train: Optional[torch.Tensor] = None

        # Normalization
        self._X_mean: Optional[torch.Tensor] = None
        self._X_std: Optional[torch.Tensor] = None
        self._y_mean: float = 0.0
        self._y_std: float = 1.0

        # Stats
        self.training_time: float = 0.0
        self.final_loss: float = float('inf')

    def _normalize_X(self, X: torch.Tensor) -> torch.Tensor:
        if self._X_mean is None:
            self._X_mean = X.mean(dim=0)
            self._X_std = X.std(dim=0).clamp(min=1e-6)

        return (X - self._X_mean.to(X.device)) / self._X_std.to(X.device)

    def _standardize_y(self, y: torch.Tensor) -> torch.Tensor:
        if self._y_mean == 0.0 and self._y_std == 1.0:
            self._y_mean = y.mean().item()
            self._y_std = y.std().item()
            if self._y_std < 1e-6:
                self._y_std = 1.0
        return (y - self._y_mean) / self._y_std

    def _destandardize_y(self, mean: torch.Tensor, std: torch.Tensor):
        return mean * self._y_std + self._y_mean, std * self._y_std

    def fit(self, X: torch.Tensor, y: torch.Tensor) -> bool:
        """Fit DKL-10D model."""
        start_time = time.time()

        # Use float32 for neural network (faster, sufficient precision)
        X = X.to(device=self.device, dtype=torch.float32)
        y = y.to(device=self.device, dtype=torch.float32)

        self.X_train = X
        self.y_train = y

        X_norm = self._normalize_X(X)
        y_norm = self._standardize_y(y)

        # Create feature extractor
        self.feature_extractor = FeatureExtractor10D(
            input_dim=X.shape[1],
            hidden_dims=self.config.hidden_dims,
            output_dim=self.config.feature_dim,
            dropout=self.config.dropout,
        ).to(self.device)

        # Create likelihood
        self.likelihood = GaussianLikelihood(
            noise_constraint=GreaterThan(1e-4),
        ).to(self.device)

        # Create DKL model
        self.model = DKL10Model(
            X_norm, y_norm, self.likelihood, self.feature_extractor
        ).to(self.device)

        # Register prior on correct device
        self.model.register_prior_on_device(self.device)

        # Training
        self.model.train()
        self.likelihood.train()

        # Joint optimizer
        optimizer = torch.optim.Adam([
            {'params': self.feature_extractor.parameters(), 'lr': self.config.lr},
            {'params': self.model.covar_module.parameters(), 'lr': self.config.lr * 10},
            {'params': self.model.mean_module.parameters(), 'lr': self.config.lr * 10},
            {'params': self.likelihood.parameters(), 'lr': self.config.lr * 10},
        ])

        mll = ExactMarginalLogLikelihood(self.likelihood, self.model)

        best_loss = float('inf')
        patience_counter = 0

        with gpytorch.settings.cholesky_jitter(self.config.jitter):
            for epoch in range(self.config.epochs):
                try:
                    optimizer.zero_grad()
                    output = self.model(X_norm)
                    loss = -mll(output, y_norm)

                    if torch.isnan(loss):
                        logger.error(f"NaN loss at epoch {epoch}")
                        return False

                    loss.backward()
                    torch.nn.utils.clip_grad_norm_(
                        list(self.feature_extractor.parameters()) +
                        list(self.model.parameters()),
                        1.0
                    )
                    optimizer.step()

                    if loss.item() < best_loss:
                        best_loss = loss.item()
                        patience_counter = 0
                    else:
                        patience_counter += 1

                    if patience_counter >= self.config.patience:
                        logger.info(f"DKL-10D early stopping at epoch {epoch + 1}")
                        break

                except RuntimeError as e:
                    if "cholesky" in str(e).lower():
                        logger.error(f"Cholesky failed at epoch {epoch}")
                        return False
                    raise

        self.training_time = time.time() - start_time
        self.final_loss = best_loss

        # Log lengthscales in 10D feature space
        ls = self.model.covar_module.base_kernel.lengthscale.detach().squeeze()
        logger.info(
            f"DKL-10D fitted in {self.training_time:.1f}s, loss={best_loss:.4f}, "
            f"feature_ls=[{ls.min():.2f}, {ls.max():.2f}]"
        )

        self.model.eval()
        self.likelihood.eval()

        return True

    def predict(self, X: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:
        """Make predictions."""
        if self.model is None:
            raise RuntimeError("Model not fitted")

        X = X.to(device=self.device, dtype=torch.float32)
        X_norm = self._normalize_X(X)

        self.model.eval()
        self.likelihood.eval()

        with torch.no_grad(), gpytorch.settings.fast_pred_var():
            pred = self.model(X_norm)
            mean_norm = pred.mean
            std_norm = pred.variance.sqrt()

        mean, std = self._destandardize_y(mean_norm, std_norm)
        return mean.clamp(0, 1), std.clamp(min=1e-6)

    def get_features(self, X: torch.Tensor) -> torch.Tensor:
        """Extract 10D features for visualization/analysis."""
        if self.feature_extractor is None:
            raise RuntimeError("Model not fitted")

        X = X.to(device=self.device, dtype=torch.float32)
        X_norm = self._normalize_X(X)

        self.feature_extractor.eval()
        with torch.no_grad():
            return self.feature_extractor(X_norm)


# =============================================================================
# Improved SAAS with Minimum Lengthscale
# =============================================================================

@dataclass
class ImprovedSAASConfig:
    """Configuration for Improved SAAS GP."""
    warmup_steps: int = 128
    num_samples: int = 64
    thinning: int = 2
    min_lengthscale: float = 1.0  # Minimum lengthscale (key improvement!)
    num_restarts: int = 32
    raw_samples: int = 512


class ImprovedSAAS:
    """SAAS GP with minimum lengthscale constraint.

    The standard SAAS can learn very small lengthscales (0.5-2.5)
    in high dimensions, leading to overconfidence. This version
    enforces a minimum lengthscale to encourage smoother models.
    """

    def __init__(
        self,
        config: Optional[ImprovedSAASConfig] = None,
        device: str = "cuda",
        fit_on_cpu: bool = True,
    ):
        try:
            from botorch.models.fully_bayesian import SaasFullyBayesianSingleTaskGP
            from botorch.fit import fit_fully_bayesian_model_nuts
            self._SaasGP = SaasFullyBayesianSingleTaskGP
            self._fit_nuts = fit_fully_bayesian_model_nuts
        except ImportError as e:
            raise ImportError(f"SAAS requires BoTorch >= 0.10.0: {e}")

        self.config = config or ImprovedSAASConfig()
        self.device = torch.device(device)
        self.fit_device = torch.device("cpu") if fit_on_cpu else self.device

        self.gp_model = None
        self.X_train: Optional[torch.Tensor] = None
        self.y_train: Optional[torch.Tensor] = None

        self._X_min: Optional[torch.Tensor] = None
        self._X_max: Optional[torch.Tensor] = None
        self._X_range: Optional[torch.Tensor] = None

        self.training_time: float = 0.0
        self.lengthscales: Optional[torch.Tensor] = None

    def _normalize_X(self, X: torch.Tensor) -> torch.Tensor:
        X = X.to(dtype=DTYPE)

        if self._X_min is None:
            self._X_min = X.min(dim=0).values
            self._X_max = X.max(dim=0).values
            range_vals = self._X_max - self._X_min
            range_vals[range_vals < 1e-6] = 1.0
            self._X_range = range_vals

        if self._X_min.device != X.device:
            self._X_min = self._X_min.to(X.device)
            self._X_max = self._X_max.to(X.device)
            self._X_range = self._X_range.to(X.device)

        return ((X - self._X_min) / self._X_range).clamp(0, 1)

    def fit(self, X: torch.Tensor, y: torch.Tensor) -> bool:
        """Fit SAAS GP with post-hoc lengthscale clamping."""
        from botorch.models.transforms.outcome import Standardize

        start_time = time.time()

        X = X.to(device=self.fit_device, dtype=DTYPE)
        y = y.to(device=self.fit_device, dtype=DTYPE)

        if X.shape[0] < 5:
            logger.error(f"Need at least 5 points, got {X.shape[0]}")
            return False

        self.X_train = X
        self.y_train = y

        X_norm = self._normalize_X(X)
        y_2d = y.unsqueeze(-1)

        logger.info(
            f"Fitting ImprovedSAAS: {X.shape[0]} points, {X.shape[1]}D, "
            f"min_ls={self.config.min_lengthscale}"
        )

        try:
            self.gp_model = self._SaasGP(
                train_X=X_norm,
                train_Y=y_2d,
                outcome_transform=Standardize(m=1),
            ).to(self.fit_device)

            self._fit_nuts(
                self.gp_model,
                warmup_steps=self.config.warmup_steps,
                num_samples=self.config.num_samples,
                thinning=self.config.thinning,
                disable_progbar=False,
            )

            # POST-HOC: Clamp lengthscales to minimum
            # This is the key improvement!
            with torch.no_grad():
                raw_ls = self.gp_model.covar_module.base_kernel.lengthscale
                clamped_ls = raw_ls.clamp(min=self.config.min_lengthscale)
                self.gp_model.covar_module.base_kernel.lengthscale = clamped_ls

                self.lengthscales = clamped_ls.median(dim=0).values.squeeze()

                logger.info(
                    f"  Lengthscales clamped to min={self.config.min_lengthscale}: "
                    f"[{self.lengthscales.min():.2f}, {self.lengthscales.max():.2f}], "
                    f"median={self.lengthscales.median():.2f}"
                )

        except Exception as e:
            logger.error(f"SAAS fitting failed: {e}")
            return False

        self.training_time = time.time() - start_time
        logger.info(f"ImprovedSAAS fitted in {self.training_time:.1f}s")

        return True

    def predict(self, X: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:
        """Make predictions."""
        if self.gp_model is None:
            raise RuntimeError("GP not fitted")

        X = X.to(device=self.fit_device, dtype=DTYPE)
        X_norm = self._normalize_X(X).clamp(0, 1)

        with torch.no_grad():
            posterior = self.gp_model.posterior(X_norm)
            mean_samples = posterior.mean.squeeze(-1)
            mean = mean_samples.mean(dim=0)

            var_epistemic = mean_samples.var(dim=0)
            var_aleatoric = posterior.variance.squeeze(-1).mean(dim=0)
            std = (var_epistemic + var_aleatoric).sqrt()

        return mean.clamp(0, 1), std.clamp(min=1e-6)


# =============================================================================
# Factory Functions
# =============================================================================

# =============================================================================
# VanillaGP with Acquisition Function (for FlowPO-HD integration)
# =============================================================================

class VanillaGPWithAcquisition:
    """VanillaGP with qLogNEI acquisition for FlowPO-HD integration.

    Provides the same interface as SaasGPWithAcquisition but uses:
    - Dimension-scaled LogNormal prior (Hvarfner 2024)
    - MLE fitting (faster than MCMC)
    - Better calibration (96% coverage vs 73% for SAAS)
    """

    def __init__(
        self,
        config: Optional[VanillaGPConfig] = None,
        device: str = "cuda",
    ):
        self.config = config or VanillaGPConfig()
        self.device = torch.device(device)

        self.model: Optional[VanillaGPModel] = None
        self.likelihood: Optional[GaussianLikelihood] = None
        self.X_train: Optional[torch.Tensor] = None
        self.y_train: Optional[torch.Tensor] = None

        # Normalization (will be set from external data)
        self._X_mean: Optional[torch.Tensor] = None
        self._X_std: Optional[torch.Tensor] = None
        self._y_mean: float = 0.0
        self._y_std: float = 1.0

        # Stats
        self.training_time: float = 0.0
        self.relevant_dims: List[int] = []

    def _normalize_X(self, X: torch.Tensor) -> torch.Tensor:
        if self._X_mean is None:
            self._X_mean = X.mean(dim=0)
            self._X_std = X.std(dim=0).clamp(min=1e-6)

        return (X - self._X_mean.to(X.device)) / self._X_std.to(X.device)

    def _standardize_y(self, y: torch.Tensor) -> torch.Tensor:
        if self._y_mean == 0.0 and self._y_std == 1.0:
            self._y_mean = y.mean().item()
            self._y_std = y.std().item()
            if self._y_std < 1e-6:
                self._y_std = 1.0
        return (y - self._y_mean) / self._y_std

    def _destandardize_y(self, mean: torch.Tensor, std: torch.Tensor):
        return mean * self._y_std + self._y_mean, std * self._y_std

    def fit(
        self,
        X: torch.Tensor,
        y: torch.Tensor,
        variances: Optional[torch.Tensor] = None,
    ) -> bool:
        """Fit VanillaGP via MLE."""
        start_time = time.time()

        X = X.to(device=self.device, dtype=DTYPE)
        y = y.to(device=self.device, dtype=DTYPE)

        if X.shape[0] < 3:
            logger.error(f"Need at least 3 points, got {X.shape[0]}")
            return False

        self.X_train = X
        self.y_train = y

        # Reset normalization for new fit
        self._X_mean = None
        self._X_std = None
        self._y_mean = 0.0
        self._y_std = 1.0

        X_norm = self._normalize_X(X)
        y_norm = self._standardize_y(y)

        dim = X.shape[1]

        # Create likelihood
        self.likelihood = GaussianLikelihood(
            noise_constraint=GreaterThan(1e-6),
        ).to(self.device, dtype=DTYPE)

        # Create model
        self.model = VanillaGPModel(
            X_norm, y_norm, self.likelihood,
            dim=dim,
            use_ard=self.config.use_ard,
            kernel_type=self.config.kernel_type,
            min_lengthscale=self.config.min_lengthscale,
        ).to(self.device, dtype=DTYPE)

        # Register prior on device
        self.model.register_prior_on_device(self.device)

        # Training
        self.model.train()
        self.likelihood.train()

        optimizer = torch.optim.Adam(self.model.parameters(), lr=self.config.lr)
        mll = ExactMarginalLogLikelihood(self.likelihood, self.model)

        best_loss = float('inf')
        patience_counter = 0

        with gpytorch.settings.cholesky_jitter(self.config.jitter):
            for epoch in range(self.config.epochs):
                try:
                    optimizer.zero_grad()
                    output = self.model(X_norm)
                    loss = -mll(output, y_norm)

                    if torch.isnan(loss):
                        logger.error(f"NaN loss at epoch {epoch}")
                        return False

                    loss.backward()
                    optimizer.step()

                    if loss.item() < best_loss:
                        best_loss = loss.item()
                        patience_counter = 0
                    else:
                        patience_counter += 1

                    if patience_counter >= self.config.patience:
                        break

                except RuntimeError as e:
                    if "cholesky" in str(e).lower():
                        logger.error(f"Cholesky failed at epoch {epoch}")
                        return False
                    raise

        self.training_time = time.time() - start_time

        # Extract top relevant dimensions (smallest lengthscales)
        ls = self.model.covar_module.base_kernel.lengthscale.detach().squeeze()
        sorted_dims = torch.argsort(ls)
        self.relevant_dims = sorted_dims[:10].tolist()

        logger.info(
            f"VanillaGP fitted in {self.training_time:.1f}s, "
            f"ls_range=[{ls.min():.1f}, {ls.max():.1f}]"
        )

        self.model.eval()
        self.likelihood.eval()

        return True

    def predict(self, X: torch.Tensor):
        """Make predictions. Returns object with .mean and .std attributes."""
        if self.model is None:
            raise RuntimeError("GP not fitted")

        X = X.to(device=self.device, dtype=DTYPE)
        X_norm = self._normalize_X(X)

        self.model.eval()
        self.likelihood.eval()

        with torch.no_grad(), gpytorch.settings.fast_pred_var():
            pred = self.model(X_norm)
            mean_norm = pred.mean
            std_norm = pred.variance.sqrt()

        mean, std = self._destandardize_y(mean_norm, std_norm)

        # Return object with .mean and .std for compatibility
        class Prediction:
            pass
        result = Prediction()
        result.mean = mean.clamp(0, 1)
        result.std = std.clamp(min=1e-6)
        return result

    def get_best_candidate(
        self,
        bounds: torch.Tensor,
        batch_size: int = 1,
        ucb_beta: float = 2.0,
    ) -> Tuple[torch.Tensor, float]:
        """Get best candidate using qLogNEI (if botorch available) or UCB (fallback)."""
        if self.model is None:
            raise RuntimeError("GP not fitted")

        bounds = bounds.to(device=self.device, dtype=DTYPE)

        # Normalize bounds
        bounds_norm = torch.stack([
            (bounds[0] - self._X_mean.to(bounds.device)) / self._X_std.to(bounds.device),
            (bounds[1] - self._X_mean.to(bounds.device)) / self._X_std.to(bounds.device),
        ])

        # Try botorch first, fall back to UCB-based sampling
        try:
            result = self._get_best_candidate_botorch(bounds, bounds_norm, batch_size)
            logger.debug("Using BoTorch qLogNEI acquisition")
            return result
        except ImportError:
            logger.info("BoTorch not available, using UCB-based acquisition")
            return self._get_best_candidate_ucb(bounds, bounds_norm, batch_size, ucb_beta)

    def _get_best_candidate_botorch(
        self,
        bounds: torch.Tensor,
        bounds_norm: torch.Tensor,
        batch_size: int,
    ) -> Tuple[torch.Tensor, float]:
        """Get candidate using BoTorch qLogNEI."""
        from botorch.acquisition.logei import qLogNoisyExpectedImprovement
        from botorch.optim import optimize_acqf
        from botorch.utils.sampling import draw_sobol_samples
        from botorch.models.gpytorch import GPyTorchModel
        from botorch.posteriors.gpytorch import GPyTorchPosterior
        import warnings

        X_norm_train = self._normalize_X(self.X_train)

        # Store references for the wrapper
        model = self.model
        likelihood = self.likelihood
        y_mean = self._y_mean
        y_std = self._y_std

        class BoTorchWrapper(GPyTorchModel):
            """Wrap VanillaGPModel for BoTorch acquisition functions.

            Note: No torch.no_grad() - gradients needed for acquisition optimization.
            """

            def __init__(inner_self):
                super().__init__()
                inner_self._model = model
                inner_self._likelihood = likelihood
                inner_self._y_mean = y_mean
                inner_self._y_std = y_std

            @property
            def num_outputs(inner_self):
                return 1

            def posterior(inner_self, X, observation_noise=False, **kwargs):
                # Model in eval mode but with gradients enabled for X
                inner_self._model.eval()
                inner_self._likelihood.eval()

                # Forward pass WITH gradients (required for botorch optimization)
                mvn = inner_self._model(X)
                if observation_noise:
                    mvn = inner_self._likelihood(mvn)

                # Destandardize (maintaining gradients)
                mean = mvn.mean * inner_self._y_std + inner_self._y_mean
                var = mvn.variance * (inner_self._y_std ** 2)

                # Create covariance from variance (diagonal)
                covar = torch.diag_embed(var)
                destd_mvn = gpytorch.distributions.MultivariateNormal(mean, covar)
                return GPyTorchPosterior(destd_mvn)

        wrapper = BoTorchWrapper()

        acq_func = qLogNoisyExpectedImprovement(
            model=wrapper,
            X_baseline=X_norm_train,
        )

        with warnings.catch_warnings():
            warnings.filterwarnings("ignore")

            try:
                candidates_norm, acq_value = optimize_acqf(
                    acq_function=acq_func,
                    bounds=bounds_norm,
                    q=batch_size,
                    num_restarts=16,
                    raw_samples=256,
                    options={"batch_limit": 5, "maxiter": 100},
                )
                acq_val = acq_value.item() if torch.is_tensor(acq_value) else acq_value
                logger.info(f"qLogNEI optimization succeeded: acq_value={acq_val:.4f}")
            except Exception as e:
                logger.warning(f"BoTorch optimize_acqf failed: {e}, falling back to Sobol sampling")
                samples = draw_sobol_samples(bounds=bounds_norm, n=512, q=1).squeeze(1)
                with torch.no_grad():
                    acq_values = acq_func(samples.unsqueeze(1))
                best_idx = acq_values.argmax()
                candidates_norm = samples[best_idx:best_idx+1]
                acq_value = acq_values[best_idx]

        candidates = candidates_norm * self._X_std.to(candidates_norm.device) + self._X_mean.to(candidates_norm.device)
        return candidates, acq_value.item() if torch.is_tensor(acq_value) else acq_value

    def _get_best_candidate_ucb(
        self,
        bounds: torch.Tensor,
        bounds_norm: torch.Tensor,
        batch_size: int,
        ucb_beta: float,
    ) -> Tuple[torch.Tensor, float]:
        """Get candidate using UCB-based acquisition (no botorch required).

        Uses Lower Confidence Bound (LCB) for minimization: LCB = mean - beta * std
        """
        # Generate Sobol samples
        n_samples = 1024
        dim = bounds.shape[1]

        # Sobol sequence in [0, 1]^d
        sobol_engine = torch.quasirandom.SobolEngine(dim, scramble=True)
        samples_01 = sobol_engine.draw(n_samples).to(device=self.device, dtype=DTYPE)

        # Scale to bounds (normalized space)
        samples_norm = bounds_norm[0] + samples_01 * (bounds_norm[1] - bounds_norm[0])

        # Evaluate LCB (Lower Confidence Bound for minimization)
        self.model.eval()
        self.likelihood.eval()

        with torch.no_grad(), gpytorch.settings.fast_pred_var():
            pred = self.model(samples_norm)
            mean_norm = pred.mean
            std_norm = pred.variance.sqrt()

        # Destandardize
        mean = mean_norm * self._y_std + self._y_mean
        std = std_norm * self._y_std

        # LCB: lower is better (minimizing error rate)
        lcb = mean - ucb_beta * std

        # Get best batch_size candidates
        best_indices = lcb.argsort()[:batch_size]
        candidates_norm = samples_norm[best_indices]
        acq_value = -lcb[best_indices[0]].item()  # Return negative for consistency (higher is better)

        # Denormalize
        candidates = candidates_norm * self._X_std.to(candidates_norm.device) + self._X_mean.to(candidates_norm.device)

        return candidates, acq_value


class VanillaFlowGuidedAcquisition:
    """Flow-guided acquisition using VanillaGP.

    Combines VanillaGP (dimension-scaled prior) with ManifoldKeeper
    velocity penalty for on-manifold optimization.
    """

    def __init__(
        self,
        config,  # FlowPOHDConfig
        manifold_keeper: Optional[nn.Module] = None,
    ):
        from flowpo_hd.config import FlowPOHDConfig

        self.config = config
        self.device = torch.device(config.device)
        self.manifold_keeper = manifold_keeper
        if manifold_keeper is not None:
            self.manifold_keeper = manifold_keeper.to(self.device)

        # Create VanillaGP
        gp_config = VanillaGPConfig(
            lr=0.1,
            epochs=500,
            patience=50,
            min_lengthscale=0.1,
        )
        self._gp = VanillaGPWithAcquisition(config=gp_config, device=config.device)

        self.lambda_penalty = config.fga_lambda_penalty
        self.manifold_time = config.fga_manifold_time

    def fit(
        self,
        X: torch.Tensor,
        y: torch.Tensor,
        variances: Optional[torch.Tensor] = None,
    ) -> bool:
        """Fit the GP to training data."""
        return self._gp.fit(X, y, variances)

    def _compute_velocity_penalty(self, x: torch.Tensor) -> torch.Tensor:
        """Compute velocity penalty from ManifoldKeeper."""
        if self.manifold_keeper is None:
            return torch.zeros(x.shape[0], device=x.device, dtype=x.dtype)

        with torch.no_grad():
            # ManifoldKeeper uses float32
            x_float32 = x.to(device=self.device, dtype=torch.float32)
            t = torch.full((x.shape[0],), self.manifold_time, device=self.device, dtype=torch.float32)
            velocity = self.manifold_keeper(t, x_float32)
            penalty = (velocity ** 2).sum(dim=-1)
            return penalty.to(dtype=x.dtype)

    def optimize(
        self,
        bounds: torch.Tensor,
        X_train: Optional[torch.Tensor] = None,
        y_train: Optional[torch.Tensor] = None,
        **kwargs,
    ) -> Tuple[torch.Tensor, float]:
        """Optimize acquisition with velocity penalty.

        Strategy:
        1. Get multiple candidates from qLogNEI (exploitation)
        2. Add Sobol samples for diversity (exploration)
        3. Score all candidates: GP mean + lambda * velocity_penalty
        4. Return best combined score
        """
        # Get candidates from qLogNEI (more candidates for better selection)
        n_qlogei = 8

        candidates, acq_value = self._gp.get_best_candidate(
            bounds=bounds,
            batch_size=n_qlogei,
        )

        # Add Sobol samples for exploration diversity
        if self.manifold_keeper is not None:
            n_sobol = 64
            dim = bounds.shape[1]
            sobol_engine = torch.quasirandom.SobolEngine(dim, scramble=True)
            sobol_01 = sobol_engine.draw(n_sobol).to(device=candidates.device, dtype=candidates.dtype)
            sobol_candidates = bounds[0] + sobol_01 * (bounds[1] - bounds[0])
            candidates = torch.cat([candidates, sobol_candidates], dim=0)

        if self.manifold_keeper is None:
            return candidates[0:1], acq_value

        # Compute velocity penalty for all candidates
        # Note: velocity penalty is computed in DENORMALIZED space (original SONAR embeddings)
        v_penalty = self._compute_velocity_penalty(candidates)

        # Get GP predictions
        pred = self._gp.predict(candidates)

        # Combined score: lower error + lambda * velocity_penalty
        # Lower is better (minimizing error rate)
        combined = pred.mean + self.lambda_penalty * v_penalty.to(pred.mean.device)

        # Log statistics
        best_idx = combined.argmin()
        logger.debug(
            f"Velocity penalty: min={v_penalty.min():.4f}, max={v_penalty.max():.4f}, "
            f"selected={v_penalty[best_idx]:.4f}"
        )
        logger.debug(
            f"Combined score: min={combined.min():.4f}, selected={combined[best_idx]:.4f}, "
            f"GP_mean={pred.mean[best_idx]:.4f}"
        )

        return candidates[best_idx:best_idx+1], acq_value


def create_vanilla_flow_guided_acquisition(config, manifold_keeper=None):
    """Factory function for VanillaFlowGuidedAcquisition."""
    return VanillaFlowGuidedAcquisition(config, manifold_keeper)


def create_vanilla_gp(device: str = "cuda", **kwargs) -> VanillaGP:
    """Create VanillaGP with dimension-scaled prior."""
    config = VanillaGPConfig(**kwargs)
    return VanillaGP(config=config, device=device)


def create_dkl10(device: str = "cuda", **kwargs) -> DKL10:
    """Create DKL with 10D features."""
    config = DKL10Config(**kwargs)
    return DKL10(config=config, device=device)


def create_improved_saas(device: str = "cuda", fit_on_cpu: bool = True, **kwargs) -> ImprovedSAAS:
    """Create Improved SAAS with minimum lengthscale."""
    config = ImprovedSAASConfig(**kwargs)
    return ImprovedSAAS(config=config, device=device, fit_on_cpu=fit_on_cpu)


# =============================================================================
# Test
# =============================================================================

if __name__ == "__main__":
    logging.basicConfig(level=logging.INFO)

    print("Testing Improved GP Models...")
    print("=" * 60)

    device = "cuda" if torch.cuda.is_available() else "cpu"
    print(f"Device: {device}")

    # Create synthetic data
    N, D = 30, 1024
    torch.manual_seed(42)
    X = torch.randn(N, D) * 0.2
    y = ((X[:, :10] ** 2).sum(dim=1) * 0.1 + 0.1).clamp(0, 1)

    print(f"\nSynthetic data: {N} points, {D}D")
    print(f"  y range: [{y.min():.3f}, {y.max():.3f}]")

    # Test each model
    models = [
        ("VanillaGP", create_vanilla_gp(device=device)),
        ("DKL-10D", create_dkl10(device=device)),
        ("ImprovedSAAS", create_improved_saas(device=device)),
    ]

    X_test = torch.randn(5, D) * 0.2

    for name, model in models:
        print(f"\n{'=' * 60}")
        print(f"Testing {name}...")
        print("=" * 60)

        success = model.fit(X, y)
        print(f"  Fit success: {success}")
        print(f"  Training time: {model.training_time:.1f}s")

        if success:
            mean, std = model.predict(X_test)
            print(f"  Predictions: mean=[{mean.min():.3f}, {mean.max():.3f}], "
                  f"std=[{std.min():.3f}, {std.max():.3f}]")

    print("\n" + "=" * 60)
    print("[OK] All tests passed!")
