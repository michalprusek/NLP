# FlowPO-HD: Manifold-Guided High-Dimensional Prompt Optimization

> **ðŸ“‹ See also:** [`FINDINGS.md`](FINDINGS.md) for experimental results and lessons learned.

## Overview

FlowPO-HD optimizes instruction prompts directly in **1024D SONAR embedding space** without compression. The key innovation is using a Flow Matching model as a "Manifold Keeper" that **regularizes** optimization to stay near the valid instruction manifold.

> **Note**: Initial design used manifold velocity as a "force direction". Experiments showed this doesn't work (see FINDINGS.md). The recommended approach is to use velocity magnitude as a **penalty** instead.

### Why This Approach?

| Aspect | lido_pp (compressed) | FlowPO-HD (direct) |
|--------|---------------------|-------------------|
| Latent dim | 128D (8:1 compression) | 1024D (no compression) |
| Compression loss | ~10% cosine loss | 0% |
| Adversarial risk | Low (smooth latent) | High (mitigated by ManifoldKeeper) |
| GP difficulty | Easy (128D) | Hard (mitigated by TuRBO-1024) |

**FlowPO-HD Advantages:**
- Full SONAR fidelity - no information loss from compression
- ManifoldKeeper prevents "adversarial examples" that decode poorly
- TuRBO trust regions handle curse of dimensionality

---

## Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         FlowPO-HD Pipeline                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚  SONAR      â”‚   â”‚ ManifoldKeeper  â”‚   â”‚   TuRBO-1024          â”‚ â”‚
â”‚  â”‚  Encoder    â”‚â”€â”€â–¶â”‚ (15M params)    â”‚â”€â”€â–¶â”‚   Trust Regions       â”‚ â”‚
â”‚  â”‚  1024D      â”‚   â”‚ Flow Matching   â”‚   â”‚   ARD scaling         â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚        â”‚                   â”‚                        â”‚               â”‚
â”‚        â”‚                   â”‚                        â”‚               â”‚
â”‚        â–¼                   â–¼                        â–¼               â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚              Flow-Guided Acquisition                         â”‚   â”‚
â”‚  â”‚   x_{k+1} = x_k + Î·Â·âˆ‡Î±_GP(x_k) + Î»Â·v_Î¸(x_k, t=0.9)          â”‚   â”‚
â”‚  â”‚                                                              â”‚   â”‚
â”‚  â”‚   âˆ‡Î±_GP: GP acquisition gradient (UCB)                      â”‚   â”‚
â”‚  â”‚   v_Î¸:   Manifold velocity (points towards valid text)      â”‚   â”‚
â”‚  â”‚   Î»:     Adaptive weight (0.5 â†’ 2.0)                        â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                              â”‚                                      â”‚
â”‚                              â–¼                                      â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚  SONAR      â”‚â—€â”€â”€â”‚ Candidate       â”‚â”€â”€â–¶â”‚   LLM Evaluation      â”‚ â”‚
â”‚  â”‚  Decoder    â”‚   â”‚ Embedding       â”‚   â”‚   GSM8K Error Rate    â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                                                                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Components

### 1. ManifoldKeeper (`manifold_keeper.py`)

MLP velocity field that learns the flow from noise to valid instruction embeddings.

**Architecture:**
```
Input: x(1024D) + t
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  TimestepEmbedding       â”‚
â”‚  t â†’ sinusoidal â†’ MLP    â”‚
â”‚  Output: 2048D           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  ManifoldResBlock Ã—3     â”‚
â”‚  x â†’ AdaLN(t) â†’ MLP â†’ +x â”‚
â”‚  1024 â†’ 2048 â†’ 1024      â”‚
â”‚  + Residual connection   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Output Projection       â”‚
â”‚  1024D velocity          â”‚
â”‚  (zero-initialized)      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Parameters:** ~15M
- Input projection: 1024 Ã— 1024 = 1M
- Time embedding: 256 Ã— 2048 Ã— 2 = 1M
- ResBlocks: 3 Ã— (1024Ã—2048 + 2048Ã—1024) = 12.5M
- Output: 1024 Ã— 1024 = 1M

**Key design:**
- **AdaLN conditioning**: Allows timestep-dependent behavior
- **Zero-init output**: Starts as identity flow for stable training
- **No bottleneck**: Full 1024D throughout (unlike autoencoder)

### 2. TuRBO-1024 (`turbo_1024.py`)

Trust region manager adapted for 1024D space.

**Parameters:**
| Parameter | Value | Rationale |
|-----------|-------|-----------|
| L_init | 0.4 | Smaller initial for high-D |
| L_max | 1.6 | Standard TuRBO |
| L_min | 0.0078 | 2^-7 |
| Ï„_succ | 3 | Expand after 3 successes |
| Ï„_fail | 128 | ceil(1024/8) for high-D |

**ARD scaling formula:**
```
L_i = Î»_i Ã— L / geom_mean(Î»)
```
- Dimensions with large lengthscale (smooth) get wider bounds
- Dimensions with small lengthscale (sensitive) get tighter bounds
- Volume preserved: âˆ L_i = L^d

### 3. Flow-Guided Acquisition (`flow_guided_acquisition.py`)

Combines GP gradient with manifold regularization.

> **âš ï¸ IMPORTANT FINDING**: Using velocity as a "force direction" doesn't work well.
> See `FINDINGS.md` for detailed experimental results.

**Recommended approach:**
```
x_{k+1} = x_k + Î·Â·âˆ‡Î±_GP(x_k) - Î»Â·penalty(x_k)
```

Where `penalty(x) = ||v_Î¸(x, t=0.9)||Â²` penalizes high velocity magnitude.

**What Works:**
- âœ… Seeding from perturbations of training data
- âœ… Velocity magnitude as soft penalty
- âœ… Proximity to training data constraint

**What Doesn't Work:**
- âŒ Using v(x, t) as direction to push towards manifold
- âŒ ODE projection of optimized embeddings
- âŒ Velocity as manifold distance metric

**Why the original approach fails:**
Flow matching learns `x_t = (1-t)Â·noise + tÂ·data` transport.
The velocity v(x, t) is only meaningful for interpolated states,
NOT for arbitrary off-manifold points or real data.

### 4. GP Configuration (SAAS + qLogEI - Benchmark Winner)

> **NEW in v2.0:** Based on GP benchmark study on 1024D SONAR space, SAAS + medium_600 achieved **Spearman 0.87** correlation between predicted and actual rankings.

**Recommended: SAAS GP with Warm-Start**
- Uses pre-evaluated HbBoPs data (~26 points with fidelity â‰¥ 600)
- SAAS (Sparse Axis-Aligned Subspaces) via MCMC identifies relevant dimensions
- qLogEI acquisition (numerically stable, better than UCB)
- No cold-start problem - GP starts with real data

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  SAAS GP Pipeline                                           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                             â”‚
â”‚  HbBoPs Results (267 evals)                                 â”‚
â”‚         â”‚                                                   â”‚
â”‚         â–¼ filter(fidelity >= 600)                          â”‚
â”‚  Medium-Fidelity Data (~26 points)                         â”‚
â”‚         â”‚                                                   â”‚
â”‚         â–¼ SONAR encode + Beta posterior smoothing           â”‚
â”‚  Warm-Start Tensors (X, y, variances)                       â”‚
â”‚         â”‚                                                   â”‚
â”‚         â–¼                                                   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  SAAS GP (Fully Bayesian)                            â”‚   â”‚
â”‚  â”‚  - NUTS MCMC (warmup=128, samples=64)               â”‚   â”‚
â”‚  â”‚  - HalfCauchy prior on lengthscales (sparsity)      â”‚   â”‚
â”‚  â”‚  - Identifies ~5-10 relevant dims out of 1024       â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚         â”‚                                                   â”‚
â”‚         â–¼                                                   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  qLogEI Acquisition                                  â”‚   â”‚
â”‚  â”‚  - Log Expected Improvement (numerically stable)    â”‚   â”‚
â”‚  â”‚  - Marginalizes over MCMC samples                   â”‚   â”‚
â”‚  â”‚  - Optional velocity penalty filter                 â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**SAAS Configuration (from benchmark):**
| Parameter | Value | Rationale |
|-----------|-------|-----------|
| warmup_steps | 128 | NUTS burn-in |
| num_samples | 64 | Posterior samples |
| thinning | 2 | Reduce autocorrelation |
| min_fidelity | 600 | medium_600 strategy |

**Benchmark Results:**
| Strategy | Spearman â†‘ | RMSE | Coverage90 |
|----------|------------|------|------------|
| SAAS + medium_600 | **0.87** | 0.020 | 96% |
| SAAS + high_1000 | 0.60 | 0.033 | 94% |
| Isotropic | -0.26 | 0.036 | 75% |

**Fallback: UCB (for debugging)**
- Î²_start = 4.0 (high exploration)
- Î²_end = 2.0 (more exploitation)
- Linear decay over iterations

---

## Training Pipeline

### Phase 1: Data Preparation

```bash
# Encode APE instructions with SONAR (unnormalized)
# Output: flowpo_hd/data/sonar_unnorm.pt
```

**Data source:** `lipo/data/ape_instructions.json` (2000 instructions)

**SONAR settings:**
- `normalize=False` (decoder requires natural magnitude ~0.18)
- `source_lang="eng_Latn"`

### Phase 2: ManifoldKeeper Training

```bash
uv run python -m flowpo_hd.training.train_manifold_keeper \
    --epochs 50000 \
    --batch-size 256 \
    --lr 1e-4
```

**Training:**
- OT-CFM loss: ||v_pred - (x_1 - x_0)||Â²
- OT pairing via Sinkhorn (GPU-friendly)
- U-shaped timestep sampling (more weight at tâ‰ˆ0, tâ‰ˆ1)
- Early stopping with patience=2000

**Target:** >90% valid instruction generation rate

### Phase 3: Optimization

```bash
uv run python -m flowpo_hd.scripts.run_flowpo_hd \
    --iterations 50 \
    --manifold-keeper-path flowpo_hd/checkpoints/best.pt
```

---

## Key Parameters

### FlowPOHDConfig

```python
@dataclass
class FlowPOHDConfig:
    # SONAR
    sonar_dim: int = 1024           # Fixed by SONAR
    sonar_normalize: bool = False   # Keep unnormalized for decoder

    # ManifoldKeeper
    mk_hidden_dim: int = 2048       # Hidden dimension
    mk_num_blocks: int = 3          # Residual blocks
    mk_time_dim: int = 256          # Timestep embedding
    mk_dropout: float = 0.1

    # TuRBO
    turbo_L_init: float = 0.4       # Smaller for 1024D
    turbo_tau_fail: int = 128       # ceil(1024/8)

    # Flow-Guided Acquisition
    fga_manifold_time: float = 0.9  # Near-clean time
    fga_lambda_start: float = 0.5   # Initial manifold weight
    fga_lambda_end: float = 2.0     # Final manifold weight
    fga_num_steps: int = 50         # Gradient steps
    fga_num_restarts: int = 32      # Random restarts

    # GP
    gp_ucb_beta_start: float = 4.0  # High exploration
    gp_ucb_beta_end: float = 2.0    # More exploitation
    gp_switch_threshold: int = 30   # Switch to SAAS
```

---

## Verification

### ManifoldKeeper Quality Test

```bash
uv run python -m flowpo_hd.scripts.evaluate_manifold
```

**Metrics:**
1. **Sample validity rate**: noise â†’ ODE â†’ decode â†’ valid English?
   - Target: >90%
2. **Reconstruction cosine**: text â†’ SONAR â†’ project â†’ decode â†’ re-encode
   - Target: >0.85
3. **Velocity quality**: does v(x, t=0.9) improve text validity?

### End-to-End Test (no LLM)

```bash
uv run python -m flowpo_hd.scripts.run_flowpo_hd \
    --iterations 10 \
    --skip-llm-eval
```

**Checks:**
- GP fits correctly
- TuRBO adapts (expand/shrink/restart)
- Candidates decode to valid text

### Full Optimization

```bash
tmux new-session -d -s flowpo_hd \
    "CUDA_VISIBLE_DEVICES=0,1 uv run python -m flowpo_hd.scripts.run_flowpo_hd \
        --iterations 50 2>&1 | tee flowpo_hd/results/run_$(date +%Y%m%d_%H%M%S).log"
```

---

## File Structure

```
flowpo_hd/
â”œâ”€â”€ __init__.py
â”œâ”€â”€ config.py                      # FlowPOHDConfig dataclass
â”œâ”€â”€ manifold_keeper.py             # MLP velocity field
â”œâ”€â”€ turbo_1024.py                  # TuRBO for 1024D
â”œâ”€â”€ flow_guided_acquisition.py     # GP + manifold optimization (UCB & SAAS)
â”œâ”€â”€ saas_gp.py                     # NEW: SAAS GP with qLogEI (benchmark winner)
â”œâ”€â”€ warm_start.py                  # NEW: HbBoPs data loading for warm-start
â”œâ”€â”€ utils.py                       # Utilities
â”œâ”€â”€ training/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ data_loader.py             # Dataset and DataLoader
â”‚   â””â”€â”€ train_manifold_keeper.py   # OT-CFM training
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ run_flowpo_hd.py           # Main optimization
â”‚   â””â”€â”€ evaluate_manifold.py       # Quality metrics
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ sonar_unnorm.pt            # SONAR embeddings (gitignored)
â”‚   â””â”€â”€ warm_start_embeddings.pt   # Cached warm-start embeddings (gitignored)
â”œâ”€â”€ checkpoints/                   # Model checkpoints (gitignored)
â”œâ”€â”€ results/                       # Run results (gitignored)
â”œâ”€â”€ PIPELINE.md                    # This file
â””â”€â”€ FINDINGS.md                    # Experimental results
```

---

## References

1. **Flow Matching**: Lipman et al., "Flow Matching for Generative Modeling" (ICLR 2023)
2. **OT-CFM**: Liu et al., "Improving the Training of Rectified Flows" (2024)
3. **TuRBO**: Eriksson et al., "Scalable Global Optimization via Local Bayesian Optimization" (NeurIPS 2019)
4. **SONAR**: Duquenne et al., "SONAR: Sentence-Level Multimodal and Language-Agnostic Representations" (2023)
5. **SAAS**: Eriksson & Jankowiak, "High-Dimensional Bayesian Optimization with Sparse Axis-Aligned Subspaces" (NeurIPS 2021)
